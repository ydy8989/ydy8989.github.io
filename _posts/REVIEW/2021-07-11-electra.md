---
layout: post
title: ELECTRA - Pre-training Text Encoders as Discriminators Rather Than Generators 논문 리뷰
subtitle: Efficiently Learning an Encoder that Classifies Token Replacements Accurately
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [NLP]
tags: [nlp, electra, machine translation]
comments: true
---

ICLR 2020에서 구글 리서치 팀이 새로운 pre-training 기법을 적용한 language model인 ELECTRA(**E**fficiently **L**earning an **E**ncoder that **C**lassifies **T**oken **R**eplacements **A**ccurately)를 발표하였다. BERT 이후의 많은 language model은 MLM task를 통해 pre-training을 하게되는데, 이런 모델들은 학습에 많은 시간과 계산량을 필요하므로 컴퓨팅 리소스가 "많이" 필요하다. 

ELECTRA는 모델의 정확도와 더불어 효율성에 포인트를 맞춘 방식의 모델이다. 본 논문에서 학습 효율을 향상하기 위해 새로운 pre-training 방식인 Replaced Token Detection(RTD)라는 방식을 제안하고, 이를 통해 보다 빠르고 효율적으로 학습한다. 

결과적으로 ELECTRA는 동일한 크기, 데이터, 컴퓨팅 리소스 조건에서 BERT의 성능을 능가하였다. Small 모델과 Large 모델을 실험하였고, 각각 GPT나 RoBERTa, XLNet 대비 동일 조건 우수한 성능에 도달하였다. 

**Paper : [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)**

> 이 글은 [핑퐁블로그](https://blog.pingpong.us/electra-review/)와 [유튜브](https://www.youtube.com/watch?v=ayVS904xQpQ&t=183s)를 참고하여 작성되었습니다. 

<br/>

# Motivation

논문 등장 기준 시점까지의 최신 언어 모델의 학습 방식은 denoising representation 방식으로 학습하는 것을 볼 수 있다. 전체 토큰 중 일부분(15%)를 마스킹하고, 이를 훈련하는 과정에서 원래 입력을 복구하게 된다. 물론 이 방식이 SOTA를 갱신하고 있으므로 좋은 방식임은 틀림없다. 또한, 기존의 순차적으로 토큰을 입력받는 방식에 비해 MLM은 양방향으로 데이터를 본다는 관점(사실 양방향이라기 보다는 문장을 한 번에 전체적으로 본다는 것이 맞겠다.)에서 Masked Language Model은 좋은 모델이다. 

그러나 기존 MLM 모델은 다음 세 가지의 문제점들을 가지고 있는데, 

1. **loss가 발생하는 지점은 마스킹된 15% 부분이 전부이다.**   
	마스킹 되는 토큰의 비율이 15%이고, 해당 부분에서만 pred와 origin을 비교하기 때문이다.
2. **상당한 컴퓨팅 리소스가 필요하다**  
	당연하게도, 15%만 학습에 사용되니 더더더더 많은 양의 데이터가 필요하고 모델이 커지게 되는 것이다. 
3. 학습 때는 **[MASK]** 토큰을 모델이 참고하여 예측하지만 실제(inference)로는 **[MASK]** 토큰이 존재하지 않는다.

![image](https://user-images.githubusercontent.com/38639633/126133773-b12f4a82-e1f2-4f0e-8c56-05fe8cdf8fe3.png)

본 논문은 학습 효율을 향상시키기 위해 **Replaced Token Detection (RTD)**이라는 새로운 pre-training 태스크를 제안하며, 이를 통해 ELECTRA는 보다 빠르고 효과적으로 학습한다고 주장한다. 또한 실험을 통해 **BERT**의 성능을 능가함을 보여주고 있으며, Small 모델 세팅에서는 GPU 하나로 4일간 학습한 모델이 계산량이 30배 가량인 **GPT**를 능가하였다고 보여주고 있다. 또한, Large 모델 세팅에서도 **RoBERTa**나 **XLNet** 대비 1/4의 계산량으로 비슷하거나 약간 향상된 성능을 보여주고 있다고 한다.

RTD는 generator를 이용해 실제 입력의 일부 토큰을 그럴싸한 가짜 토큰으로 바꾸고, 각 토큰이 실제 입력에 있는 진짜(*original*) 토큰인지 generator가 생성해낸 가짜(*replaced*) 토큰인지 discriminator가 맞히는 이진 분류 문제라고 할 수 있다. RTD Task로 ELECTRA는 입력의 15%가 아니라 100%의 토큰을 모두 학습에 사용하기 때문에 효율적이고 효과적이다. (~~사실 이게 정상이지 않나...싶다~~) 

![image](https://user-images.githubusercontent.com/38639633/126276775-66c19060-5cbc-46b3-869d-a32b38e3b570.png){:width="80%"}{:.center}

위와 같이 다른 방식의 MLM 모델에 비하여 ELECTRA가 빠르게 성능이 향상되는 것을 볼 수 있다. 특히, 동일 조건(모델 사이즈, 데이터, 계산량(FLOPs))에서 더 높은 GLUE 성능을 보인다. 

<br/>

# Method

![image](https://user-images.githubusercontent.com/38639633/126278601-71cd5ffa-a948-45ce-96a9-dfab1bec6b35.png){:width="80%"}{:.center}



먼저 ELECTRA 학습을 위한 RTD task를 살펴보면, 위 그림과 같이 Generator $G$와 Discriminator $D$로 구성되어 있다. $G$와 $D$는 모두 Transformer의 encoder 부분으로 구성되어 있으며, 뒤에서 설명하겠지만, $G$는 하나의 Transformer, $D$는 두 개의 Transformer로 구성되어있다. $G$는 기존의 MLM 모델과 같은 방식으로 학습된다(아래 step 1~4).  $D$는 ELECTRA에서 새롭게 추가된 부분에 해당하는데, 입력 토큰 시퀀스($G$를 통과한 토큰 시퀀스이며, 원본 토큰 시퀀스가 아니라, G가 생성한 Fake 토큰 시퀀스이다. )에서 각 토큰이 원본 토큰과 같은지 아닌지를 binary classification하는 구조이다.   
Input부터 output까지의 전체 과정을 step by step으로 설명하면, 

1. 입력 $\textbf{x} = [x_1, x_2, …, x_n]$를 받아 마스킹할 위치의 집합 $\textbf{m} = [m_1, m_2, …, m_k]$를 결정한다. 이때, 위치 $m_1,\dots, m_k$는 균등분포  $$unif \{ 1,n \}$$를 따른다. (여기서 $k$는 **0.15**$n$ = 전체 토큰 갯수 $n$개 중 15%)

2. 여기까지 공통된 부분이고, 이 입력 $\textbf{x} = [x_1, x_2, …, x_n]$와 $\textbf{m} = [m_1, m_2, …, m_k]$은  $G$와 $D$로 각각 나뉘어 들어가게 되는데 이 부분은 뒤에서 설명하도록 한다. 

3.  $\textbf{m} = [m_1, m_2, …, m_k]$의 위치에 해당하는 입력 토큰을 $[MASK]$ 토큰으로 치환한다.
	
	$$
	\textbf{x}^{masked}=\text{REPLACE}(\textbf{x}, \textbf{m}, [MASK])
	$$

	위와 같이 $\textbf{x}$를 $\textbf{m}$의 위치에 대하여 $[MASK]$로 바꾼다는 의미의 수식으로 표기한다. 

4. $G$에서는 마스킹 된 입력 토큰 $\textbf{x}^{masked}$에 대해서 원래 토큰이 무엇인지를 예측한다.
   
	$$
	p_G (x_t | \textbf{x}^{masked}) = \exp(e(x_t)^T h_G(\textbf{x}^{masked})_t) / \sum_{x'} \exp(e(x')^T h_G(\textbf{x}^{masked})_t)
	$$

	수식은 위와 같이 표기될 수 있다. $\textbf{x}^{masked}$가 주어졌을 때 토큰의 위치 $x_t$의 분포는 $G$를 통과한 Softmax 분포로 표현됨을 알 수 있다. 여기서 $e(\cdot)$은 임베딩을 의미한다. 이후 아래와 같이 MLM Loss를 통해 학습을 진행한다. 

	$$
	\mathcal{L}_{\text{MLM}}(\textbf{x}, \theta_G) = \mathbb{E} \left( \sum_{i \in \textbf{m}} -\log p_G (x_i | \textbf{x}^{masked}) \right)
	$$
   
	$G$의 학습 stage는 위의 loss를 구하면서 여기서 끝이 나지만, $D$의 입력 토큰 시퀀스는 앞서 계산한 $p_G (x_t \vert\textbf{x}^{masked})$에서 sampling을 통해 얻게된다. 

5. 앞서 step2에서 언급 했듯이 준비된 마스킹 위치 $\textbf{m}$에 대하여 $\textbf{x}$를 $G$에서는 $[MASK]$로 바꿨다면, 여기서는 $p_G (x_t \vert\textbf{x}^{masked})$에서 Sampling한 토큰으로 교체한다. 즉,

	$$
	\textbf{x}^{corrupt}=\text{REPLACE}(\textbf{x}, \textbf{m}, \hat{\textbf{x}})\\
	\hat{\textbf{x}}\sim p_G (x_i | \textbf{x}^{masked}) \; \text{for} \; i \in \textbf{m}
	$$
	
	으로 표현이 가능하다. 조금 더 설명하자면, $G$에서 구한 softmax의 분포로부터 $x_t$위치의 토큰에 들어갈 가장 높은 확률 값을 지니는 토큰($\hat{\textbf{x}}$)을 샘플링하고, 마스킹하는 대신 해당 토큰으로 치환하는 것이다. 

6. 이렇게 치환된 토큰 시퀀스 $\textbf{x}^{corrupt}$에 대해서 $D$는 각 토큰이 원래 입력 토큰 시퀀스와 동일한지 예측한다. 여기서 기존 방식과의 차이점은 마스킹 위치의 $\textbf{m}$에 대한 것만 진행하는 것이아니라 전체 FULL 토큰에 대하여 예측을 진행한다. 수식은 아래와 같이 표현 가능하다  
	
	$$
	D(\textbf{x}^{corrupt}, t) = \text{sigmoid}(w^T h_D(\textbf{x}^{corrupt})_t)
	$$

	$t$번째 토큰에 대한 예측을 나타낸다. 

7. 최종적으로는 아래와 같은 Loss로 학습한다.   
	$$
	\begin{align}
	&\mathcal{L}_{Disc} (\textbf{x}, \theta_{D})\\ 
	& =\mathbb{E} \left( \sum_{t=1}^{n} -\mathbb{1}(x_{t}^{corrupt} = x_t) \log D(\textbf{x}^{corrupt}, t) - \mathbb{1}(x_{t}^{corrupt} \neq x_t) \log (1-D(\textbf{x}^{corrupt}, t)) \right)
	\end{align}
	$$
	



