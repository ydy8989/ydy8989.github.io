---
layout: post
title: ELECTRA - Pre-training Text Encoders as Discriminators Rather Than Generators 논문 리뷰
subtitle: Efficiently Learning an Encoder that Classifies Token Replacements Accurately
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [NLP]
tags: [nlp, electra, machine translation]
comments: true
---

ICLR 2020에서 구글 리서치 팀이 새로운 pre-training 기법을 적용한 language model인 ELECTRA(**E**fficiently **L**earning an **E**ncoder that **C**lassifies **T**oken **R**eplacements **A**ccurately)를 발표하였다. BERT 이후의 많은 language model은 MLM task를 통해 pre-training을 하게되는데, 이런 모델들은 학습에 많은 시간과 계산량을 필요하므로 컴퓨팅 리소스가 "많이" 필요하다. 

ELECTRA는 모델의 정확도와 더불어 효율성에 포인트를 맞춘 방식의 모델이다. 본 논문에서 학습 효율을 향상하기 위해 새로운 pre-training 방식인 Replaced Token Detection(RTD)라는 방식을 제안하고, 이를 통해 보다 빠르고 효율적으로 학습한다. 

결과적으로 ELECTRA는 동일한 크기, 데이터, 컴퓨팅 리소스 조건에서 BERT의 성능을 능가하였다. Small 모델과 Large 모델을 실험하였고, 각각 GPT나 RoBERTa, XLNet 대비 동일 조건 우수한 성능에 도달하였다. 

**Paper : [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)**

> 이 글은 [핑퐁블로그](https://blog.pingpong.us/electra-review/)와 [유튜브](https://www.youtube.com/watch?v=ayVS904xQpQ&t=183s)를 참고하여 작성되었습니다. 



# 1. Motivation

논문 등장 기준 시점까지의 최신 언어 모델의 학습 방식은 denoising representation 방식으로 학습하는 것을 볼 수 있다. 전체 토큰 중 일부분(15%)를 마스킹하고, 이를 훈련하는 과정에서 원래 입력을 복구하게 된다. 물론 이 방식이 SOTA를 갱신하고 있으므로 좋은 방식임은 틀림없다. 또한, 기존의 순차적으로 토큰을 입력받는 방식에 비해 MLM은 양방향으로 데이터를 본다는 관점(사실 양방향이라기 보다는 문장을 한 번에 전체적으로 본다는 것이 맞겠다.)에서 Masked Language Model은 좋은 모델이다. 

그러나 기존 MLM 모델은 다음 세 가지의 문제점들을 가지고 있는데, 

1. **loss가 발생하는 지점은 마스킹된 15% 부분이 전부이다.** 

	마스킹 되는 토큰의 비율이 15%이고, 해당 부분에서만 pred와 origin을 비교하기 때문이다.

2. **상당한 컴퓨팅 리소스가 필요하다**

	당연하게도, 15%만 학습에 사용되니 더더더더 많은 양의 데이터가 필요하고 모델이 커지게 되는 것이다. 

3. 학습 때는 **[MASK]** 토큰을 모델이 참고하여 예측하지만 실제(inference)로는 **[MASK]** 토큰이 존재하지 않는다.

![image](https://user-images.githubusercontent.com/38639633/126133773-b12f4a82-e1f2-4f0e-8c56-05fe8cdf8fe3.png)

본 논문은 학습 효율을 향상시키기 위해 **Replaced Token Detection (RTD)**이라는 새로운 pre-training 태스크를 제안하며, 이를 통해 ELECTRA는 보다 빠르고 효과적으로 학습한다고 주장한다. 또한 실험을 통해 **BERT**의 성능을 능가함을 보여주고 있으며, Small 모델 세팅에서는 GPU 하나로 4일간 학습한 모델이 계산량이 30배 가량인 **GPT**를 능가하였다고 보여주고 있다. 또한, Large 모델 세팅에서도 **RoBERTa**나 **XLNet** 대비 1/4의 계산량으로 비슷하거나 약간 향상된 성능을 보여주고 있다고 한다.

RTD는 generator를 이용해 실제 입력의 일부 토큰을 그럴싸한 가짜 토큰으로 바꾸고, 각 토큰이 실제 입력에 있는 진짜(*original*) 토큰인지 generator가 생성해낸 가짜(*replaced*) 토큰인지 discriminator가 맞히는 이진 분류 문제라고 할 수 있다. RTD Task로 ELECTRA는 입력의 15%가 아니라 100%의 토큰을 모두 학습에 사용하기 때문에 효율적이고 효과적이다. (~~사실 이게 정상이지 않나...싶다~~) 

![image](https://user-images.githubusercontent.com/38639633/126176524-68d8a7eb-aa77-4cea-b55e-d19e3c05739a.png)

위와 같이 다른 방식의 MLM 모델에 비하여 빠르게 성능이 향상되는 것을 볼 수 있다. 특히, 동일 조건(모델 사이즈, 데이터, 계산량(FLOPs))에서 더 높은 GLUE 성능을 보인다. 

