---
layout: post
title: ELECTRA - Pre-training Text Encoders as Discriminators Rather Than Generators 논문 리뷰
subtitle: Efficiently Learning an Encoder that Classifies Token Replacements Accurately
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [NLP]
tags: [nlp, electra, machine translation]
comments: true
---

ICLR 2020에서 구글 리서치 팀이 새로운 pre-training 기법을 적용한 language model인 ELECTRA(**E**fficiently **L**earning an **E**ncoder that **C**lassifies **T**oken **R**eplacements **A**ccurately)를 발표하였다. BERT 이후의 많은 language model은 MLM task를 통해 pre-training을 하게되는데, 이런 모델들은 학습에 많은 시간과 계산량을 필요하므로 컴퓨팅 리소스가 "많이" 필요하다. 

ELECTRA는 모델의 정확도와 더불어 효율성에 포인트를 맞춘 방식의 모델이다. 본 논문에서 학습 효율을 향상하기 위해 새로운 pre-training 방식인 Replaced Token Detection(RTD)라는 방식을 제안하고, 이를 통해 보다 빠르고 효율적으로 학습한다. 

결과적으로 ELECTRA는 동일한 크기, 데이터, 컴퓨팅 리소스 조건에서 BERT의 성능을 능가하였다. Small 모델과 Large 모델을 실험하였고, 각각 GPT나 RoBERTa, XLNet 대비 동일 조건 우수한 성능에 도달하였다. 

**Paper : [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)**

> 이 글은 [핑퐁블로그](https://blog.pingpong.us/electra-review/)와 [유튜브](https://www.youtube.com/watch?v=ayVS904xQpQ&t=183s)를 참고하여 작성되었습니다. 



# 1. Motivation

MLM

