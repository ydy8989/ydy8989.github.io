---
layout: post
title: REVIEW / Exploring randomly wired neural networks for image recognition 
subtitle: 랜덤하게 신경망을 만들자
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [REVIEW]
tags: [paper, review, graph theory, random graph, NAS]
comments: true
---
첫 논문 리뷰 포스팅을 이 논문으로 하는 이유는 별 이유 없다. 단지, 내 전공이 그래프 이론이기 때문이다. 
작년 상반기에 페이스북 tensorflow kr 그룹 페이지에서 핫했던 논문이다(지금은 사람들의 관심에서 멀어진 듯하다). 
아무튼 수학을 전공하고 독학 및 자잘한 교육으로 인공지능을 공부하던 나에겐 반가운 분야였다.\
논문에서는 세 가지 그래프 생성 모델을 통해 신경망을 디자인하고, 여러 실험으로 마무리하였다. 

### Abstract
장점은 
1. 웨이티드 썸 : 들어오는 채널 같다고 가정할때 채널수를 동일하게 맞출 수 있음
2. 그래서 모든 노드와도 연결할 수 있음
3. 연산량이나 파라미터도 노드에서 변하지 않게 할 수 있음. 노드에 달린 엣지의 갯수에 상관없어서
4. 전체 연산량도 그러므로 노드의 갯수에 비례할뿐임. 단지 웨이트의 갯수에 따라 약간의 차이는 발생할 수 있음
5. 즉 결국, 연산량과 채널 고정하고 와이얼리하게 연결만 다르게해서 ... 모든 노드에 접근 가능함.
    
<br/> 

### 인풋 and 아웃풋 노드. 

- 완전랜덤이 아님. 
- 랜덤으로 생성한거 예를들어 시작이 1,2번 마지막 랜덤 아웃풋 노드가 3456 이면, 앞뒤단에 인위적인 노드 한개씩 추가해서 연결함. 
- ![1](/assets/img/network.png)
- 아웃풋 엑스트라 노드에서는 웨이티드썸이아니라 에버리지. 평균을 냄

<br/>

### Stages 

- 스트라이드처럼. 랜덤 제너레이티드 된 하나의 그래프는 하나의 스테이지를 만듬. 
- 스테이지들 간에는 엑스트라 인풋/아웃풋 노드와연결함
- 하나의 스테이지를 건너게 되면 스트라이드 2로하고 채널은 두배로 해줌.

<br/>


### 실험

- 모바일넷, 셔플넷이랑 비교
- resnet 이랑 비교. 
- 노드랑 채널 수를 정해야하는데, 각각 노드32 채널 79, 109로 했다고함. 

<br/>

### 결과

- 망한 모델은 없다
-  ![2](/assets/img/accuracy.png)

- 적어도 다 73% 이상을 기록했다고함
-  표준편차도 작다고함 : 0.2~0.4%정도
- 각 모델별 평균치가 중요하기 때문에 랜덤제너레이터를 어케 디장인하는지가 중요하다..라고 말하고 있음

<br/>

#### 번외 : 그래프 데미지 실험

- 그래프에서 랜덤하게 엣지나 노드 하나를 없애버림.
- ![3](/assets/img/degree.png)
- 위쪽이 노드없앤거, 아래가 엣지 없앤거.
- 노드 없엔거에서는 아웃풋 디그리가 x축.
- 뭔말이냐면 노드에서 디그리 높은 노드가 없어질수록 데미지 많이 입느다.. 뭐 이런

---

- 엣지 데미지 경우는 살짝 반대. 디그리가 큰 노드에서 엣지를 하나 빼면 타격 덜입는다 뭐 ...

<br/>

### 노드의 Operation

- 노드 하나당 한ㄴ 종류의 오퍼레이션 셋 
- 그 종류만 바꿔서 랜덤으로 만들어본결과임
- 

![4](/assets/img/model.png)

<br/> 

### 성능?

- 몇종류 빼고는 거의다 높거나 비슷
- 연상량과 파라미터 수도 비슷
- 레즈넷도 다.... 
- 라지 컴퓨테이션, 오브젝트 디텍션의 백본으로 사용해도 다 우수했다고함

### reference
> NASNet 설명 : <http://research.sualab.com/review/2018/09/28/nasnet-review.html>
