---
layout: post
title: RoBERTa - A Robustly Optimized BERT Pretrining Approach 논문 리뷰
subtitle: BERT를 더 강건하게
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [NLP]
tags: [nlp, roberta, machine translation]
comments: true

---

Roberta : A Robustly Optimized BERT Pretraining Approach는 이름에서 알 수 있듯  



## Roberta 논문 요약

1. 모델 학습시간 증가, 배치사이즈 늘리고 트레인 데이터 증가
	1. 버트를 조금 더 강하게 만든다생각함
	2. 프리트레인에서 데이터 양을 늘릴수록 다운 스트림 태스크 성능이 증가
2. 넥스트 sentence prediction 제거
3. longer sequence 추가
4. 마스킹 패턴 다이내믹하게 추가
	1. 버트는 pretrain 전에 미리 masking진행
	2. 학습 진행 때 똑같은 token이 마스킹 된다는 문제 있음 bias
	3. 똑같은 데이터에 대해 마스킹 10번 다르게 적용
	4. 인풋 ㄷ르어갈때마다 마스킹 진행



---

[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)

### Abstract

bert가 좋은 성능을 냈지만, 아직 underfit인 상황이라고 가정한다. 하이퍼파라미터를 다시 선택하고, replication 스터디를 진행한다. training data도 다시 재정의하고 학습함



### introduction

논문에서 제시하고자 하는것은 다음 4가지임

1. 모델 더 오래학습, 큰 배치를 넣어주고, 더 많은 데이터
2. NSP(next sentence prediction)를 없애봄 
3. longer sequence를 넣어줌
4. 마스크 패턴 다이내믹하게 

다시 요약하면

1. 버트 성능 위해 training 전략 다시 잡음
2. 새 데이터(큰 데이터)를 통해 다운스트림 태스크 성능에 도움되는 것을 증명함
3. MLM pretraining 모델이 짱임



### Background

버트 정리함



### Experimental Setup

**Implementation**

기본적으로 Bert의 training hyperparameter 따름

Peak learning rate랑 warm up step 변경

adam의 epsilon 변경하고 $\beta_2$ 변경 



**Data**

160GB의 데이터 사용



### Training Procedure Analysis

BERT base와 같은 사이즈로 픽스해두고 진행했다고 한다.

같은 masking만 계속해서 쓰는 것을 방지하기 위해 training data를 10개로 복사하고 10가지 방식으로 masking을 했다. 그리고 40 epoch로 학습을 했으니 같은 masking을 네번만 보게함

static masking을 구현했을 때는 BERT랑 거의 같았고, dynamic masking을 했을 때에는 거의 비슷하거나 static masking보다 조금 더 나았다.



