---
layout: post
title: NLP / Attention & beamsearch and BLEU
subtitle: ì§‘ì¤‘ì˜ ì‹œì‘
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [BOOSTCAMP]
tags: [boostcamp, attention]
comments: true
---



# Sequence to Sequence with Attention

Sequenceë¥¼ Encodingì™€ Decodingí•  ìˆ˜ ìˆëŠ” **sequence to sequence**ì— ëŒ€í•´ ì•Œì•„ë´…ë‹ˆë‹¤.

**Sequence to sequence**ëŠ” encoderì™€ decoderë¡œ ì´ë£¨ì–´ì ¸ ìˆëŠ” frameworkìœ¼ë¡œ ëŒ€í‘œì ì¸ ìì—°ì–´ ì²˜ë¦¬ architecture ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. Encoderì™€ Decoderë¡œëŠ” ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì´ ì‚¬ìš©ë  ìˆ˜ ìˆì§€ë§Œ ì´ë²ˆ ì‹œê°„ì—ëŠ” **RNNê³¼ Attention**ì„ ê²°í•©í•œ sequence to sequence ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

ì•ì„  ê°•ì˜ì—ì„œ ì„¤ëª…ë“œë ¸ë˜ ê²ƒì²˜ëŸ¼ RNN ëª¨ë¸ì´ ê°–ê³  ìˆëŠ” ë‹¨ì ì„ ë³´ì™„í•˜ê³ ì **Attention**(ë…¼ë¬¸ì—ì„œëŠ” alignmentë¡œ í‘œí˜„ë˜ê³  ìˆìŠµë‹ˆë‹¤) ê¸°ë²•ì´ ì²˜ìŒ ë“±ì¥í–ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ Attentionì˜ ì¢…ë¥˜ì™€ ì´ë¥¼ í™œìš©í•œ translation taskì— ëŒ€í•´ì„œ ì•Œì•„ë´…ë‹ˆë‹¤

**Further Reading**

- [Sequence to sequence learning with neural networks, ICMLâ€™14](https://arxiv.org/abs/1409.3215)
- [Effective Approaches to Attention-based Neural Machine Translation, EMNLP 2015](https://arxiv.org/abs/1508.04025)
- [CS224n(2019)_Lecture8_NMT](https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf)

<br>

## Seq2Seq with attention

### Seq2Seq Model

- It takes a `sequence of words` as input and gives `a sequence of words as output`
- It composed of an `encoder` and a `decoder`

![image](https://user-images.githubusercontent.com/38639633/108185462-aafd1580-714f-11eb-9311-cf58203c8b6a.png){:.center}

> Sequence to sequence learning with neural networks, ICMLâ€™14

<br>

### Seq2Seq Model with Attention

- attentionì€ encoder decoderì˜ `bottleneck` ë¬¸ì œë¥¼ í•´ê²°í•˜ì˜€ë‹¤.
- **Core idea :** Decoderì˜ ê° timestepì— ëŒ€í•˜ì—¬, src sequenceì˜ íŠ¹ì •í•œ ë¶€ë¶„ì— ì§‘ì¤‘í•œë‹¤ëŠ” ì•„ì´ë””ì–´

![img](https://3.bp.blogspot.com/-3Pbj_dvt0Vo/V-qe-Nl6P5I/AAAAAAAABQc/z0_6WtVWtvARtMk0i9_AtLeyyGyV6AI4wCLcB/s1600/nmt-model-fast.gif){:.center}

- Use the attention distribution to take a weighted sum of the encoder hidden states
- The attention output mostly contains information the hidden states that received high attention

![attention](../../assets/img/boostcamp/attention.gif)

- Concatenate attention output with decoder hidden state, then use to compute $\hat{y}_1$as before
	- ì²« ë²ˆì§¸ Decoder hidden state $h_1^{(d)}$ëŠ” Encoder hidden stateë“¤ì˜ concatenate $[h_1^{(e)},h_2^{(e)},h_3^{(e)},h_4^{(e)}]$ì™€ Matrix ì—°ì‚°ì„ ì§„í–‰í•œë‹¤.
	- ìœ„ ê·¸ë¦¼ì—ì„œëŠ” 4 by 4 matrix(concatenate of encoder hidden state)ì™€ 4 by 1 matrix(decoder hidden state)ë¥¼ ê³±í•˜ê²Œ ëœë‹¤. 
	- ê·¸ ê²°ê³¼ë¡œ ë‚˜ì˜¨ 4 by 1 matrix(vector)ëŠ” encoderì˜ ê° timestepì— í•´ë‹¹í•˜ëŠ” `attention scores`ê°€ ëœë‹¤. 
	- ì´ë ‡ê²Œ ê³„ì‚°ëœ attention scoreëŠ” softmaxë¥¼ ê±°ì¹˜ê²Œ ëœë‹¤. 
	- ê·¸ ê²°ê³¼ ê° tokenì— í•´ë‹¹í•˜ëŠ” ê°€ì¤‘ì¹˜ ë²¡í„°ë¥¼ ì–»ê²Œë˜ê³ , ì´ ê°€ì¤‘ì¹˜ì™€ encoder hidden stateë¥¼ ë°˜ì˜í•œ `ê°€ì¤‘í‰ê· `ì„ ë°”íƒ•ì„ Attention output vector(Context vector)ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. 
	- ì´ë•Œ, `ATTENTION MODULE`ì€ encoder hidden stateë¡œ ë¶€í„° êµ¬í•´ì§€ëŠ” Attention scoreì™€ Attention distribution(softmax output) ë‘ ë¶€ë¶„ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆë‹¤.
	- Attention moduleì˜ inputê³¼ outputì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
		- input : decoderì˜ hidden state, encoder hidden stateì˜ concatenate
		- output : ê°€ì¤‘í‰ê· ìœ¼ë¡œ ê³„ì‚°ëœ output vector 1ê°œ
	- `output layer`ëŠ” Context vectorì™€ decoderì˜ hidden stateë¥¼ concatenateí•œ ë²¡í„°($\hat{y}_1$)ê°€ Inputìœ¼ë¡œ ë“¤ì–´ê°„ë‹¤. 
- Training : Decoderì˜ inputìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ë‹¨ì–´ë“¤ì€ ground truthë¡œ ë“¤ì–´ê°€ê²Œ ëœë‹¤. (Teacher forcing ë°©ì‹)
- Inference : ì´ë•ŒëŠ” ì²« ë²ˆì§¸ì˜ output ë‹¨ì–´ë¥¼ ë‹¤ì‹œ ë‘ë²ˆì§¸ input ë‹¨ì–´ë¡œ ì‚¬ìš©í•œë‹¤. 
- Teacher forcingì˜ ê²½ìš°, ì†ë„ëŠ” ë¹ ë¥´ì§€ë§Œ ì‹¤ì œ ì‚¬ìš©í–ˆì„ ë•Œ ê´´ë¦¬ê°€ ìˆì„ ìˆ˜ ìˆë‹¤. 
<br>

### Different Attention Mechanisms

- **`Luong attention`**: they get the decoder hidden state at time ğ‘¡, then calculate attention scores, and from that get the context vector which will be concatenated with hidden state of the decoder and then predict the output. 
- **`Bahdanau attention`**: At time **t**, we consider the hidden state of the decoder at time **t âˆ’ 1**. Then we calculate the alignment, context vectors as above. But then we concatenate this context with hidden state of the decoder at time t âˆ’ 1. So before the softmax, this concatenated vector goes inside a LSTM unit. 
- **Luong** has different types of alignments. **Bahdanau** has only a concat-score alignment model.

![image](https://user-images.githubusercontent.com/38639633/108210628-23270380-716f-11eb-8ce9-6bfedc98ca07.png){:.center}

- ë‹¤ì–‘í•œ ë°©ì‹ì˜ attention scoreë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ì‹ì´ ì¡´ì¬í•œë‹¤. 
<br>

### Attention is Great!

- Attention significantly improves NMT performance
	- It is useful to allow the decoder to focus on particular parts of the source
- Attention solves the bottleneck problem
	- Attention allows the decoder to look directly at source; bypass the bottleneck
- Attention helps with vanishing gradient problem
	- Provides a shortcut to far-away states
- Attention provides some interpretability
	- By inspecting attention distribution, we can see what the decoder was focusing on
	- The network just learned alignment by itself
<br>

### Attention Examples in Machine Translation

- It properly learns grammatical orders of words
- It skips unnecessary words such as an article

![image](https://user-images.githubusercontent.com/38639633/108213315-38516180-7172-11eb-9635-cc7d93a84b46.png){:.center}
<br>

# Beam Search and BLEU

**ê°•ì˜ ì†Œê°œ**

ë¬¸ì¥ì„ decoding í•˜ëŠ” ë°ì— ì‚¬ìš©í•˜ëŠ” ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜ì¸ **Beam Search**ì™€ ë²ˆì—­ taskì—ì„œ ë²ˆì—­ëœ ë¬¸ì¥ì„ í‰ê°€í•˜ëŠ” ëŒ€í‘œì ì¸ metricì¸ **BLEU score**ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.

ì–¸ì–´ ëª¨ë¸ì´ ë¬¸ì¥ì„ generationí•  ë•Œì—ëŠ” í™•ë¥ ê°’ì— ê¸°ë°˜í•œ ë‹¤ì–‘í•œ ê²½ìš°ì˜ ìˆ˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ë¥¼ ê³ ë ¤í•˜ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì ì´ë©° ë„ˆë¬´ ì‘ì€ í™•ë¥ ê°’ê¹Œì§€ ê³ ë ¤í•œë‹¤ë©´ ìƒì„±ëœ ë¬¸ì¥ì˜ qualityê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ì¥ ë†’ì€ í™•ë¥ ê°’ì„ ê³ ë ¤í•˜ëŠ” ë°©ë²• ì—­ì‹œ ëª¨ë¸ì´ ë‹¨ìˆœí•œ generationì„ í•˜ë„ë¡ í•˜ê²Œ ë§Œë“œëŠ” ë‹¨ì ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œì˜ ëŒ€ì•ˆìœ¼ë¡œ ì œì•ˆëœ Beam Searchë¥¼ ì•Œì•„ë´…ë‹ˆë‹¤.

ìì—°ì–´ëŠ” ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ë°©ì‹ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ëª¨ë¸ì˜ ì…ë ¥ ë° ì¶œë ¥ìœ¼ë¡œ í™œìš©ë˜ê¸° ë•Œë¬¸ì— ì ì ˆí•œ metricì„ ì´ìš©í•´ ëª¨ë¸ì„ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ìì—°ì–´ì²˜ë¦¬ ê´€ë ¨ metricì´ ìˆì§€ë§Œ, ê·¸ì¤‘ì—ì„œë„ ë²ˆì—­ taskì—ì„œ ê°€ì¥ ëŒ€í‘œì ì¸ BLEU scoreë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ë²ˆì—­ì— ìˆì–´ì„œ BLEU scoreê°€ precisionì„ ê³ ë ¤í•˜ëŠ” ì´ìœ ì— ëŒ€í•´ì„œ ê³ ë¯¼í•˜ë©´ì„œ ê°•ì˜ë¥¼ ë“¤ì–´ì£¼ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

**Further Reading**

- [Deep learning.ai-BeamSearch](https://www.youtube.com/watch?v=RLWuzLLSIgw&feature=youtu.be)
- [Deep learning.ai-RefiningBeamSearch](https://www.youtube.com/watch?v=gb__z7LlN_4&feature=youtu.be)
- [OpenNMT-beam search](https://opennmt.net/OpenNMT/translation/beam_search/)

**Further Question**

- BLEU scoreê°€ ë²ˆì—­ ë¬¸ì¥ í‰ê°€ì— ìˆì–´ì„œ ê°–ëŠ” ë‹¨ì ì€ ë¬´ì—‡ì´ ìˆì„ê¹Œìš”?
   - ì°¸ê³ : [Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics](https://arxiv.org/abs/2006.06264?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+arxiv%2FQSXk+%28ExcitingAds%21+cs+updates+on+arXiv.org%29)

<br>


## Beam search

ìì—°ì–´ ìƒì„±ëª¨ë¸ì—ì„œ í…ŒìŠ¤íŠ¸ íƒ€ì„ì‹œ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚´ê¸° ìœ„í•œ ë°©ë²•ì¸ `beam search`ë¥¼ ì•Œì•„ë³´ì. 
<br>

### Greedy approach

í…ìŠ¤íŠ¸ ìƒì„±ì‹œ ëª¨ë¸ì€ ê°€ì¥ ë†’ì€ í™•ë¥ ë¡œ ë“±ì¥í•  ë‹¨ì–´ë¥¼ ì¶œë ¥í•˜ê²Œ ëœë‹¤. ì´ ë°©ë²•ì€ ì „ì²´ì ì¸ êµ¬ì¡°ì—ì„œì˜ í™•ë¥ ê°’ì„ ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê·¼ì‹œì•ˆì ìœ¼ë¡œ í˜„ì¬ timestepì—ì„œ ê°€ì¥ ì¢‹ì•„ë³´ì´ëŠ” ë‹¨ì–´ë¥¼ ì„ íƒí•˜ëŠ” í˜•íƒœë¥¼ `Greedy approach`ë¼ê³  í•œë‹¤.

- Greedy decoding has no way to undo decisions!
   - input : il a mâ€™entartÃ© (he hit me with a pie)
      - he \___
      - he hit \___
      - he hit `a` \___ ~~(whoops, no going back nowâ€¦)~~

ìœ„ ì˜ˆì‹œì—ì„œ `hit` ë‹¤ìŒ `me`ê°€ ë‚˜ì™€ì•¼í•  ì°¨ë¡€ì— `a`ê°€ ë‚˜ì™”ë‹¤ê³  í–ˆì„ ë•Œ, ì´ì–´ ë‚˜ì˜¤ëŠ” ëª¨ë“  ì˜ˆì¸¡ì€ í‹€ì–´ì§€ê²Œ ëœë‹¤. ê·¸ ì´ìœ ëŠ” ë’¤ì—ì„œ ì‚¬ìš©í•´ì•¼í•  ë‹¨ì–´ë¥¼ ì´ë¯¸ ì•ì—ì„œ ìƒì„±í•´ë²„ë ¸ê¸° ë•Œë¬¸ì´ë‹¤. ì´ë¯¸ ë’¤ëŠ¦ê²Œ ê¹¨ë‹¬ì•˜ë‹¤ê³  í•˜ë”ë¼ë„ ì´ë¯¸ ê³ ì •ëœ ì˜ˆì¸¡ê°’ì€ ë°”ê¿€ ìˆ˜ ì—†ë‹¤. 


<br>

### Exhaustive search

***Then how can we fix this?***

- Ideally, we want to find a (length $T$) translation $y$ that maximizes. 
   $$
   \begin{align}
   P(y\vert x)
   &=P(y_1\vert x)P(y_2\vert y_1, x)P(y_3\vert y_2, y_1, x)\dots P(y_T\vert y_1, \dots , y_{T-1}, x)\\
   &= \prod_1^TP(y_t\vert y_1, \dots , y_{t-1}, x)
   \end{align}
   $$

- We could try computing **all possible sequences** $y$

   - This means that on each step $t$ of the decoder, we are tracking $V^t$ possible partial translations, where $V$ is the vocabulary size
   - This $O(V^t)$ complexity is far too expensive!

ì¦‰, joint probabilityì—ì„œ ìˆœì°¨ì ìœ¼ë¡œ ê°ê°ì˜ ìµœëŒ€ë¥¼ ê³±í•œ ê°’ì´ ì „ì²´ì˜ ìµœëŒ€ëŠ” ì•„ë‹ ìˆ˜ë„ ìˆë‹¤ëŠ” ê²ƒì´ë©° ì•ì—ì„œ ì•½ê°„ì˜ ì†í•´(ì‘ì€ í™•ë¥ ê°’)ë¥¼ ë³´ë”ë¼ë„ ë’¤ì—ì„œ ë§ŒíšŒí•  ìˆ˜ ìˆëŠ” í™•ë¥ ê°’ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì´ ì•„ì´ë””ì–´ì˜ í•µì‹¬ì´ë‹¤. 

í•˜ì§€ë§Œ, ë§¤ timestepë§ˆë‹¤ ëª¨ë“  ê°€ì§“ìˆ˜ë¥¼ ê³ ë ¤í•œë‹¤ë©´ ë„ˆë¬´ ë§ì€ ê³„ì‚°ëŸ‰ì´ í•„ìš”í•˜ë‹¤. 


<br>

### Beam search

ë””ì½”ë”ì˜ ë§¤ íƒ€ì„ë§ˆë‹¤ ì ì ˆí•œ $k$ê°œì˜ ê°€ì§“ìˆ˜ë¥¼ ê³ ë ¤í•˜ê³ , ê·¸ ì¤‘ì—ì„œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ê²ƒì„ íƒí•˜ëŠ” ë°©ë²•ì´ë‹¤. 

> Core idea: on each time step of the decoder, we keep track of the $k$ most probable partial translations (which we call hypothese)
>
> - $k$ is the beam size (in practice around 5 to 10)
>
> A hypothesis $y_1, \dots, y_t$ has a score of its log probability:
> $$
> score(y_1, \dots, y_t) = logP_{LM}(y_1, \dots, y_t\vert x)=\sum^t_{i=1}logP_{LM}(y_i\vert y_1, \dots, y_{t-1},x)
> $$
> Scores are all negative, and a higher score is better  
> We search for high-scoring hypotheses, tracking the top k ones on each step  

logë¥¼ ì”Œì›€ìœ¼ë¡œì¨ ê³±ì…ˆì„ ë§ì…ˆì—°ì‚°ìœ¼ë¡œ ë°”ê¾¸ê³ , ì¶”ì²™í•˜ê¸° ì‰½ë„ë¡ í•˜ëŠ” ê²ƒì´ ëª©í‘œ

- Beam search is not guaranteed to find a globally optimal solution.
- But it is much more efficient than exhaustive search!

ëª¨ë“  ê²½ìš°ë¥¼ ë‹¤ ë³´ëŠ” ê²ƒì€ ì•„ë‹ˆì§€ë§Œ, í›„ë³´êµ°ì„ ì¶”ë¦¬ê¸° ì‰½ë‹¤.!!!

**example)**

![beamsearch](../../assets/img/boostcamp/beamsearch.gif){:width="80%"}{:.center}

> ì¶œì²˜ : [https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf](https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf)

ìœ„ì™€ ê°™ì´ k=2ì¼ ë•Œ, ê³„ì†ì ìœ¼ë¡œ í™•ë¥ ì´ ë†’ì€ í›„ë³´êµ°ì„ ì°¾ìœ¼ë©´ì„œ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•œë‹¤. 
<br>

### Beam search: Stopping criterion

- Usually we continue beam search until:
  - We reach timestep ğ‘‡ (where ğ‘‡ is some pre-defined cutoff), or 
  - We have at least ğ‘› completed hypotheses (where ğ‘› is the pre-defined cutoff)


<br>

### Beam search: Finishing up

- We have our list of completed hypotheses
- How to select the top one with the highest score?
- Each hypothesis $ğ‘¦_1, â€¦ , ğ‘¦_t$ on our list has a score

$$
score(y_1, \dots, y_t) = logP_{LM}(y_1, \dots, y_t\vert x)=\sum^t_{i=1}logP_{LM}(y_i\vert y_1, \dots, y_{t-1},x)
$$

- Problem with this : **longer hypotheses have lower scores**
- Fix : Normalize by length

$$
score(y_1, \dots, y_t) = \frac{1}{t}\sum^t_{i=1}logP_{LM}(y_i\vert y_1, \dots, y_{t-1},x)
$$

<br>


## BLEU score

ìì—°ì–´ ìƒì„± ëª¨ë¸ì—ì„œ ìƒì„± ëª¨ë¸ì˜ í’ˆì§ˆ ë° ê²°ê³¼ë¥¼ í‰ê°€í•˜ëŠ” scoring ë°©ì‹ì„ ì•Œì•„ë³´ì