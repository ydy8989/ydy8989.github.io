---
layout: post
title: NLP / NaiveBayes and Embedding
subtitle: NLPë¥¼ ì‹œì‘í•´ë³´ì
thumbnail-img : https://user-images.githubusercontent.com/38639633/107952158-bbd94a00-6fdc-11eb-957f-385b8d281d01.png 
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [BOOSTCAMP]
tags: [boostcamp]
comments: true
---
ìì—°ì–´ ì²˜ë¦¬ì˜ ì²« ì‹œê°„ìœ¼ë¡œ NLPì— ëŒ€í•´ ì§§ê²Œ ì†Œê°œí•˜ê³  ìì—°ì–´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì¸ **Bag-of-Words**ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. Bag-of-WordsëŠ” ë‹¨ì–´ì˜ í‘œí˜„ì— ìˆì–´ì„œ one-hot-encodingì„ ì´ìš©í•˜ë©°, ë‹¨ì–´ì˜ ë“±ì¥ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ” ì•„ì£¼ ê°„ë‹¨í•œ ë°©ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ê°„ë‹¨í•œ ëª¨ë¸ì´ì§€ë§Œ ë§ì€ ìì—°ì–´ ì²˜ë¦¬ taskì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³ , ì´ Bag-of-Wordsë¥¼ ì´ìš©í•´ ë¬¸ì„œë¥¼ ë¶„ë¥˜í•˜ëŠ” **Naive Bayes Classifier**ì— ëŒ€í•´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ë²ˆ ê°•ì˜ì—ì„œëŠ” **ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•, ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•**ì— ëŒ€í•´ ê³ ë¯¼í•´ë³´ë©´ì„œ ê°•ì˜ë¥¼ ë“¤ì–´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.

<br>

# 1. Intro to NLP, Bag-of-Words
## Intro to Natural Language Processing(NLP)

### ì´ë²ˆ ê³¼ì •ì˜ ëª©í‘œ

- Natural language processing (NLP), which aims at properly understanding and generating
	human languages(NLG), emerges as a crucial application of artificial intelligence, with the
	advancements of deep neural networks.
- This course will cover various deep learning approaches as well as their applications such as
	language modeling, **machine translation, question answering, document classification, and dialog systems**. 

<br>

###  í•™ë¬¸ì  ì²´ê³„

- NLP(ì£¼ìš” í•™íšŒ : ACL, EMNLP, NAACL)
	- low level parsing
		- í† í¬ë‚˜ì´ì§•, stemming(ì–´ë¯¸ì˜ ë³€í™”ì— ëŒ€í•œ ì—°êµ¬, ì–´ê·¼ ì¶”ì¶œ)
	- word and phrase level
		- ê°œì²´ëª… ì¸ì‹(NER) : ë‹¨ì¼ ë‹¨ì–´ í˜¹ì€ ì—¬ëŸ¬ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ ê³ ìœ ëª…ì‚¬ë¥¼ ì¸ì‹í•˜ëŠ” ì‘ì—…
		- POS(part-of-speech) tagging : ë¬¸ì¥ë‚´ì—ì„œ ì›Œë“œì˜ í’ˆì‚¬ë‚˜ ì„±ë¶„ì´ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë‚´ëŠ” task
		- noun-phrase chunking 
		- dependecy íŒŒì‹±
		- coreference resolution 
	- Sentence level
		- ê°ì •ë¶„ì„(sentiment analysis)
		- machine translation
	- Multi-sentence and paragraph level
		- Entailment prediction : ë‘ ë¬¸ì¥ê°„ì˜ ë…¼ë¦¬ì  ë‚´í¬ ë° ëª¨ìˆœê´€ê³„ ì¶”ë¡ 
		- question answering  : ë…í•´ ê¸°ë°˜ì˜ ì§ˆì˜ì‘ë‹µ
		- dialog systems : ëŒ€í™”ëª¨ë¸, ì±—ë´‡ì„.
		- summarization
- Text mining(KDD, The webconf(formerly, WWW), WSDM, CIKM, IWSM)
	- ë¹…ë°ì´í„°ì™€ ì—°ê´€ëœ ê²½ìš°ê°€ ë§ë‹¤. 
	- Extract useful information and insights from text and document data
	- Document clustering (e.g., topic modeling)
	- Highly related to computational social science - íŠ¸ìœ„í„°ë‚˜ ì†Œì…œ ë¯¸ë””ì–´ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬íšŒí˜„ìƒ ë“±ë“±ì„ ë¶„ì„í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤. 
- Information retrieval - ì •ë³´ê²€ìƒ‰ ë¶„ì•¼ (ì£¼ìš” í•™íšŒ : SIGIR, WSDM, CIKM, RecSys)
	- Highly related to computational social science
		- ì´ë¯¸ ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ê³ ë„í™” ë˜ì–´ ìƒëŒ€ì ìœ¼ë¡œ ì—°êµ¬ê°€ ë”ë”˜ ë¶„ì•¼ì´ë‹¤. 
		- í•˜ì§€ë§Œ, ì¶”ì²œì‹œìŠ¤í…œ ë¶„ì•¼ëŠ” í™œë°œíˆ ì—°êµ¬ë˜ê³  ìˆë‹¤. 

<br>

### NLP ë¶„ì•¼ì˜ íŠ¸ë Œë“œ

- ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë‚˜íƒ€ë‚´ëŠ” í…Œí¬ë‹‰ w2v or glove
- RNN ê³„ì—´ì˜ ëª¨ë¸
- attention moduleì— ê¸°ë°˜í•œ transformer ëª¨ë¸
- ê° NLP taskì— ë§ëŠ” ì„¸ë¶€ì ì¸ ëª¨ë¸ ì„¤ê³„ë¡œ ë¶„í™”ë¨
- self-supervised í•™ìŠµëœ ëª¨ë¸ë“¤(bert, gpt-3)ì€ íŠ¹ì • íƒœìŠ¤í¬ì—ì„œ ë²—ì–´ë‚˜ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ë¡œ ë°œì „í•˜ì˜€ë‹¤. 
- ëª¨ë¸ì´ ì»¤ì§ìœ¼ë¡œ ì¸í•œ ìì›ì  í•œê³„ë¡œ ì „ì´í•™ìŠµì´ íŠ¸ë Œë“œê°€ë¨

<br>

## Bag of Words

### Bag-of-Words Representation

- Step 1. ìœ ë‹ˆí¬í•œ ë‹¨ì–´ë¥¼ ëª¨ì•„ì„œ vocabì„ êµ¬ì¶•í•œë‹¤.

	-  Example sentences: â€œJohn really really loves this movieâ€œ, â€œJane really likes this songâ€
	-  Vocabulary: {â€œJohnâ€œ, â€œreallyâ€œ, â€œlovesâ€œ, â€œthisâ€œ, â€œmovieâ€œ, â€œJaneâ€œ, â€œlikesâ€œ, â€œsongâ€}

- Step 2. ê° ë‹¨ì–´ë¥¼ one-hot ë²¡í„°ë¡œ í‘œí˜„í•œë‹¤.

	- Vocabulary: {â€œJohnâ€œ, â€œreallyâ€œ, â€œlovesâ€œ, â€œthisâ€œ, â€œmovieâ€œ, â€œJaneâ€œ, â€œlikesâ€œ, â€œsongâ€}

		![image](https://user-images.githubusercontent.com/38639633/107896991-49844d80-6f7b-11eb-9c7e-ef232de8ef90.png)

	- ì„ì˜ì˜ ë‘ ë‹¨ì–´ìŒì˜ **ìœ í´ë¦¬ë””ì–¸ distance**ëŠ” $\sqrt 2$ ì´ê³ , **ì½”ì‚¬ì¸ ìœ ì‚¬ë„**ëŠ” ëª¨ë‘ 0ì´ë‹¤ 

	- ì¦‰, ë‹¨ì–´ì˜ ì˜ë¯¸ì™€ ìƒê´€ì—†ì´ ëª¨ë‘ ë™ì¼í•˜ê²Œ ì„¤ì •ëœë‹¤. 

- ë¬¸ì¥/ë¬¸ì„œëŠ” ì´ëŸ¬í•œ one-hot ë²¡í„°ë“¤ì˜ í•©ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. 

	![image](https://user-images.githubusercontent.com/38639633/107897153-c6afc280-6f7b-11eb-9631-79ab2cd67ac6.png)

<br>

### NaiveBayes Classifier for Document Classification

ìœ„ì™€ ê°™ì´ Bag of Words ë²¡í„°ë¡œ ë‚˜íƒ€ë‚¸ ë¬¸ì„œë¥¼ ì •í•´ì§„ ì¹´í…Œê³ ë¦¬ í˜¹ì€ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ëŒ€í‘œì ì¸ ë°©ë²•ì¸ NaiveBayes Classifierë¥¼ ì•Œì•„ë³´ì

![image](https://user-images.githubusercontent.com/38639633/107897302-10001200-6f7c-11eb-97a8-eca67706a481.png){:width="80%"}{: .center}

- Bayesâ€™ Rule Applied to Documents and Classes

	- For a document d and a class c

		![image](https://user-images.githubusercontent.com/38639633/107927387-4d839000-6fba-11eb-90be-6443b810a5d9.png)

	- For a document `d`, which consists of a sequence of words `w`, and a class `c`

	- The probability of a document can be represented by multiplying the probability of each word appearing

	- $P(d\vert c)P(c)=P(w_1, w_2,\dots,w_n\vert c)P(c)\rightarrow P(c)\prod_{w_i\in W}P(w_i\vert c)$ 

	- íŠ¹ì • ì¹´í…Œê³ ë¦¬ cê°€ ê³ ì •ë˜ì—ˆì„ ë•Œ, ë¬¸ì„œ dê°€ ë‚˜íƒ€ë‚  í™•ë¥ ì´ê³ , ì´ëŠ” $w_1$ë¶€í„° $w_n$ê¹Œì§€ ë™ì‹œì— ë‚˜íƒ€ë‚  ë™ì‹œì‚¬ê±´ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. ê° ë‹¨ì–´ê°€ ë“±ì¥í•  í™•ë¥ ì´ ì„œë¡œ ë…ë¦½ì´ë©´, ì´ë¥¼ ê³±í•œ í˜•íƒœë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. 

- Example

	![image](https://user-images.githubusercontent.com/38639633/107929544-2084ac80-6fbd-11eb-9ff9-e79e11777934.png)

	ì´ëŸ¬í•œ ìƒí™©ì—ì„œ Test ë°ì´í„°ì˜ *Classification task usses transformer*ë¼ëŠ” ë¬¸ì¥ì˜ ê° ë‹¨ì–´ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

	![image](https://user-images.githubusercontent.com/38639633/107929512-1662ae00-6fbd-11eb-9f1d-0712dac98a25.png)

	For a test document $ğ‘‘_5$ = â€œClassification task uses transformerâ€

	-  We calculate the conditional probability of the document for each class
	-  We can choose a class that has the highest probability for the document

- $P(C_{cv}\vert d_5)=P(C_{CV})\prod_{w\in W}P(w\vert c_{CV})=\frac{1}{2}\times\frac{1}{10}\times\frac{1}{10}\times\frac{1}{10}\times\frac{1}{10}=0.00005$ ì´ë‹¤.

<br>

# 2. Word Embedding


ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ë²•ì¸ **Word2Vec**ê³¼ **GloVe**ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.

Word2Vecê³¼ GloVeëŠ” ìµœê·¼ê¹Œì§€ë„ ìì£¼ ì‚¬ìš©ë˜ê³  ìˆëŠ” word embedding ë°©ë²•ì…ë‹ˆë‹¤. Word2Vecê³¼ GloVeëŠ” í•˜ë‚˜ì˜ ì°¨ì›ì— ë‹¨ì–´ì˜ ëª¨ë“  ì˜ë¯¸ë¥¼ í‘œí˜„í•˜ëŠ” one-hot-encodingê³¼ ë‹¬ë¦¬ ë‹¨ì–´ì˜ distributed representationì„ í•™ìŠµí•˜ê³ ì ê³ ì•ˆëœ ëª¨ë¸ì…ë‹ˆë‹¤. Word2Vecê³¼ GloVeê°€ ë‹¨ì–´ë¥¼ í•™ìŠµí•˜ëŠ” ì›ë¦¬ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê°•ì˜ë¥¼ ë“¤ì–´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤

**Further Reading**

- [Word2Vec, NeurIPS'13](https://arxiv.org/abs/1310.4546)
- [GloVe, EMNLP'14](https://www.aclweb.org/anthology/D14-1162/)

**Further Questions**

- Word2Vecê³¼ GloVe ì•Œê³ ë¦¬ì¦˜ì´ ê°€ì§€ê³  ìˆëŠ” ë‹¨ì ì€ ë¬´ì—‡ì¼ê¹Œìš”?

---

## What is Word Embedding?

- Express a word as a vector
- 'cat' and 'kitty' are similar words, so they have similar vector representations $\rightarrow$ short distance
- 'hamburger' is not similar with 'cat' or 'kittyâ€™, so they have different vector representations $\rightarrow$ far distance
- ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ ë‹¨ì–´ê°€ ë²¡í„° ê³µê°„ ìƒì—ì„œ ê°€ê¹Œìš´ ê³³ì— ìœ„ì¹˜í•˜ê²Œë” ë¶€ì—¬í•˜ëŠ” ê²ƒì— ìˆë‹¤. 

<br>



## Word2Vec

- ì¸ì ‘ ë‹¨ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë§¥ì˜ ë‹¨ì–´ ë²¡í„°ë¥¼ í•™ìŠµí•œë‹¤. 
- **ê°€ì • :** ë¹„ìŠ·í•œ ë§¥ë½ì˜ ë‹¨ì–´ëŠ” ì˜ë¯¸ê°€ ë¹„ìŠ·í•˜ë‹¤.

### Word2Vecì˜ Idea

í•œ ë‹¨ì–´ê°€ ì£¼ë³€ì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ì„ í†µí•´ ê·¸ ì˜ë¯¸ë¥¼ ì•Œ ìˆ˜ìˆë‹¤ëŠ” ê°€ì •ì— ì°©ì•ˆí•˜ë¯€ë¡œ, ì£¼ì–´ì§„ í•™ìŠµ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì›í•˜ëŠ” ë‹¨ì–´ì˜ ì£¼ë³€ í™•ë¥  ë¶„í¬ë¥¼ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤. 

- **Distributional Hypothesis:** The meaning of â€œcatâ€ is captured by the probability distribution $P(\mathbb{w}\vert cat)$

![image](https://user-images.githubusercontent.com/38639633/107930783-c71d7d00-6fbe-11eb-86ed-0267c60939c7.png)

- ìœ„ì™€ ê°™ì´ 'Cat'ì´ë¼ëŠ” ë‹¨ì–´ ì£¼ìœ„ì— "meow", "Pet" ë“±ì˜ ë‹¨ì–´ê°€ ë†’ì€ í™•ë¥ ë¡œ ë“±ì¥í•œë‹¤ëŠ” ê²ƒì„ í•™ìŠµí•œë‹¤. 

<br>

### How Word2Vec Algorithm Works

- Sentence : â€œI study math.â€ì„ ìƒê°í•´ë³´ì

- í† í¬ë‚˜ì´ì§•ì„ í†µí•´ Vocabulary: {â€œIâ€, â€œstudyâ€ â€œmathâ€}ìœ¼ë¡œ ì‚¬ì „ì„ êµ¬ì¶•í•œë‹¤.

- ì‚¬ì „ì˜ ì‚¬ì´ì¦ˆë§Œí¼ì˜ dimensionì„ ê°€ì§€ëŠ” one-hot-vectorë¡œ vocabì„ í‘œí˜„í•œë‹¤.

- ì´í›„ Sliding Windowë¼ëŠ” ê¸°ë²•ì„ í†µí•´ ì¤‘ì‹¬ë‹¨ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì•/ë’¤ ë‹¨ì–´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” pairë¥¼ í‘œí˜„í•œë‹¤.

- vocab ì‚¬ì´ì¦ˆê°€ ì´ ê²½ìš° 3ì´ë¯€ë¡œ ì…ì¶œë ¥ ë…¸ë“œ ìˆ˜ëŠ” 3ê°œë¡œ ë˜ê³ , íˆë“  ë ˆì´ì–´ì˜ ë…¸ë“œ ìˆ˜ëŠ” ì‚¬ìš©ìê°€ ì§€ì •í•œë‹¤. ì‚¬ì˜ë˜ëŠ” ë²¡í„°ê³µê°„ì˜ dimensionê³¼ ê°™ì€ í¬ê¸°ë¥¼ ì§€ë‹Œë‹¤. 

	![image](https://user-images.githubusercontent.com/38639633/107932850-5a57b200-6fc1-11eb-80ad-5eed289b7eb0.png)

- Hidden layerì˜ ì°¨ì›ì„ 2ì°¨ì›ì´ë¼ê³  ê°€ì •í•˜ì.

- Input node to Hidden layerë¥¼ í‘œí˜„í•˜ëŠ” $\mathbf{W}_1$ëŠ” 3ì°¨ì›ì—ì„œ 2ì°¨ì›ìœ¼ë¡œ í‘œí˜„ë˜ê³ , ë‹¤ì‹œ ouput layerë¡œ ê°„ëŠ” $\mathbf{W}_2$ëŠ” 2ì°¨ì›ì—ì„œ 3ì°¨ì›ìœ¼ë¡œ í‘œí˜„ë˜ë¯€ë¡œ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

	![image](https://user-images.githubusercontent.com/38639633/107933384-07322f00-6fc2-11eb-981e-62c0af483185.png)

- [0, 1, 0]ë¡œ í‘œí˜„ë˜ëŠ” ë‹¨ì–´ â€œstudyâ€ ê°€ input, [0, 0, 1]ë¡œ í‘œí˜„ë˜ëŠ” ë‹¨ì–´  â€œmathâ€ ê°€ target vectorë¼ê³  í•  ë•Œ, ìœ„ì™€ ê°™ì´ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. 

<br>

### Property of Word2Vec

- The word vector, or the relationship between vector points in space, represents the relationship between the words.

- The same relationship is represented as the same vectors.

- ë²¡í„°ê°„ì˜ ì—°ì‚°ì´ ë‹¨ì–´ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ë°˜ì˜í•˜ì—¬ í‘œí˜„ë˜ëŠ” ê²ƒì´ íŠ¹ì§•ì´ë‹¤. 

	![image](https://user-images.githubusercontent.com/38639633/107933815-97707400-6fc2-11eb-8dfb-8bea0d6b8633.png)

<br>

### Property of Word2Vec â€“ Intrusion Detection

Word2Vecì„ í†µí•´ì„œ í•  ìˆ˜ ìˆëŠ” ë˜ ë‹¤ë¥¸ Taskì¤‘ í•˜ë‚˜ : intrusion detection

- ì—¬ëŸ¬ ë‹¨ì–´ë“¤ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ë‚˜ë¨¸ì§€ ë‹¨ì–´ì™€ ê·¸ ì˜ë¯¸ê°€ ê°€ì¥ ìƒì´í•œ ë‹¨ì–´ë¥¼ ì°¾ëŠ” task
- ë‹¨ì–´ë³„ë¡œ ë‚˜ë¨¸ì§€ ë‹¨ì–´ì™€ì˜ euclidean distanceë¥¼ ê³„ì‚°í•˜ê³  í‰ê· ê°’ì„ ê³„ì‚°í•˜ì—¬ ì°¨ì´ë¥¼ ë°œê²¬í•œë‹¤. 

<br>

### Application of Word2Vec

NLP taskì— Word2Vecì„ ì´ìš©í•œ ë‹¤ì–‘í•œ ì‘ìš©ì´ ì´ë¤„ì§€ê³  ìˆë‹¤. 

- Word similarity
- Machine translation
- Part-of-speech (PoS) tagging
- Named entity recognition (NER)
- Sentiment analysis
- Clustering
- Semantic lexicon building

<br>

## Glove

ê° ì…ë ¥, ì¶œë ¥ ë‹¨ì–´ ìŒì— ëŒ€í•˜ì—¬, í•™ìŠµ ë°ì´í„°ì—ì„œ ë‘ ë‹¨ì–´ê°€ í•œ ìœˆë„ìš°ì—ì„œ ë™ì‹œì— ëª‡ ë²ˆ ë“±ì¥í–ˆëŠ”ì§€ë¥¼ ì‚¬ì „ì— ê³„ì‚°í•˜ê³ , ì…ë ¥ì›Œë“œì˜ ì„ë² ë”©ë²¡í„°ê°„ì˜ ë‚´ì ê°’ì´ ë‘ ë‹¨ì–´ê°€ í•œ ìœˆë„ìš° ë‚´ì—ì„œ ëª‡ ë²ˆ 'ë™ì‹œì—' ë‚˜íƒ€ë‚¬ëŠ”ê°€, ê·¸ ê°’ì— ë¡œê·¸ë¥¼ ì·¨í•´ fitting ë  ìˆ˜ ìˆë„ë¡ í•™ìŠµí•˜ëŠ” ë°©ì‹ì´ë‹¤. 

- Rather than going through each pair of an input and an output words, it first computes the co-occurrence matrix, to avoid training on identical word pairs repetitively.
- Afterwards, it performs matrix decomposition on this co-occurrent matrix.

$$
J(\theta) = \frac{1}{2}\sum^\mathbf W _{i,j=1}f(P_{ij})(u^T_iv_j-logP_{ij})^2
$$

**Word2Vec**: íŠ¹ì •í•œ ì…ì¶œë ¥ ë‹¨ì–´ ìŒì´ ìì£¼ ë“±ì¥í–ˆì„ ë•Œ, ì´ ê°™ì€ ë°ì´í„° ì•„ì´í…œì´ ì—¬ëŸ¬ë²ˆì— í•™ìŠµ ë¨ìœ¼ë¡œì¨ ë‘ ì›Œë“œ ì„ë² ë”© ë‚´ì ê°’ì´ ë¹ˆë²ˆí•´ì§€ë©´ì„œ ì»¤ì§€ëŠ” ë°©ì‹ì´ë¼ë©´

**Glove** : ì• ì´ˆì— ë™ì‹œì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ ìŒì´ ë™ì‹œì— ë“±ì¥í•˜ëŠ” íšŸìˆ˜ë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•˜ê³  ì´ì—ëŒ€í•œ ë¡œê·¸ê°’ì„ ì·¨í•œ ê·¸ ê°’ì„ ì§ì ‘ í•´ë‹¹ ë‘ ë‹¨ì–´ê°„ì˜ ë‚´ì ê°’ê³¼ ì–¼ë§ˆë‚˜ ì°¨ì´ë‚˜ëŠ”ì§€ë¥¼ lossë¡œ í•˜ì—¬ í•™ìŠµí•œë‹¤. ë”°ë¼ì„œ ì¤‘ë³µë˜ëŠ” ê³„ì‚°ì„ ì¤„ì—¬ì£¼ëŠ” ì ì—ì„œ ë¹ ë¥´ê³ , ì ì€ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ì˜ ë™ì‘í•˜ëŠ” íŠ¹ì„±ì„ ì§€ë‹Œë‹¤. 

<br>

### Linear Substructure

Gloveì—ì„œë„ Word2Vecì˜ Linear Substructureì˜ ê²°ê³¼ì²˜ëŸ¼ ë²¡í„°ì— ë”°ë¥¸ wordê°„ì˜ ì°¨ì´ê°€ ìœ ì‚¬í•˜ê²Œ ì ìš©ë¨ì„ ë³¼ ìˆ˜ ìˆë‹¤. 

