---
layout: post
title: 부스트캠프 AI Tech - Computer Vision
subtitle: CNN과 Convolution
thumbnail-img : /assets/img/boostcamp/sementic.png
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [BOOSTCAMP]
tags: [boostcamp]
comments: true
---

# 1. Modern CNN - about 1x1 convolution

ILSVRC라는 Visual Recognition Challenge와 대회에서 수상을 했던 **5개 Network** 들의 주요 아이디어와 구조에 대해 배웁니다.

**Network list**

- **AlexNet**
	- 최초로 Deep Learning을 이용하여 ILSVRC에서 수상.
- **VGGNet**
	- 3x3 Convolution을 이용하여 Receptive field는 유지하면서 더 깊은 네트워크를 구성.
	- [Receptive field 참고 자료](https://cs231n.github.io/convolutional-networks/#conv)
- **GoogLeNet**
	- Inception blocks 을 제안.
- **ResNet**
	- Residual connection(Skip connection)이라는 구조를 제안.
	- h(x) = f(x) + x 의 구조
- **DenseNet**
	- Resnet과 비슷한 아이디어지만 Addition이 아닌 Concatenation을 적용한 CNN.

 

## 1.1. ILSVR

Imagenet Large-Scale Visual Recognition Challenge
- classification / detection / localization / segmentation
- 1000diff categories
- 1 million images

	| year  | Error Rate |
	| :---: | :--------: |
	| 2010  |    28%     |
	| 2011  |    25%     |
	| 2012  |    16%     |
	| 2013  |    11%     |
	| 2104  |     6%     |
	| 2015  |    3.5%    |
	| Human | about 5.1% |



## 1.2. AlexNet

**key ideas**

- ReLU activation
- 2 GPUs
- local response normalization, overlapping pooling
- data augmentation
- dropout

**relu activation** 

- preserves properties of linear model
- easy to optimize with gradient descent
- good generalization
- overcome the vanishing gradient problem



<br>



## 1.3. VGGNet

![image-20210203213902867](../../assets/img/boostcamp/image-20210203213902867.png){: .center}

**key ideas**

- 3 X 3 convolution. filter의 사이즈를 줄이기 시작했다. 
- why? 

  ![image-20210203213624646](../../assets/img/boostcamp/image-20210203213624646.png){: width = "70%"}

  - 3 by 3 filter를 두 번 사용하는 것은 5 by 5 filter를 한 번 사용하는 것과 I/O의 관점에서 별 차이가 없다. 
  - 하지만 파라미터의 수를 계산해보면 약 10만개 가량이 차이난다. 
  - 이는 계산속도와 generalization performance에 영향을 끼친다.

<br>



## 1.4. Googlenet

![image-20210203213931647](../../assets/img/boostcamp/image-20210203213931647.png){: .center}

**inception blocks**
- what are the **benefits** of the `inception block`?
	
	- reduce the number of parameter
	
- How?
  - recall how the number of parameters is computed
  - 1 X 1 convolution can be seen as channel wise dimension reduction, 
    
      > 즉 채널 방향으로 차원을 줄일 수 있다. 

  ![image-20210203214117290](../../assets/img/boostcamp/image-20210203214117290.png){:.center}

  - 앞서 설명한 "작은 사이즈의 filter를 여러번 사용하는 방법"과 유사하게,   
  	layer는 깊게 만들고 I/O size는 유지하면서 파라미터의 사이즈는 줄일 수 있다. 
  - 약 30% 가량으로 파라미터가 줄어든 모습을 확인할 수 있다. 
  
  

**benefit of 1,1 conv** 
- 1x1 convolution enables about 30% reduce of the number of pararmeters

<br>

## 1.5. comparing

**\# of parameters?**

- `AlexNet(8-layers)` : 60M
- `VGGNet(19-layers)` : 110M
- `GoogLeNet(22-layers)` : 4M

<br>

## 1.6. ResNet

**deeper neural networks are hard to train**

- overfitting is usually caused by an excessive number of parameters
- bus not in this case.

**idea**

- add an identity map(skip connection)

  

  ![image-20210203215048189](../../assets/img/boostcamp/image-20210203215048189.png){:.center}
  ![image-20210203215118424](../../assets/img/boostcamp/image-20210203215118424.png){:.center}



- skip-connection을 사용하지 않을 때(**좌측**)는 더 깊은 신경망의 error가 더 큰 모습을 보여준다. 하지만,  ResNet의 경우(**우측**), 신경망의 깊이가 깊어졌음에도 불구하고 얕은 신경망에 비해서 더 낮은 error율을 보여준다.   
	이는 신경망이 깊어졌음에도 학습을 제대로 하고 있다는 것을 보여준다. 
- add an identity map after nonlinear activations:
	- 차원을 맞춰주기 위해 1x1 conv를 이용한다. 
- bottleneck architecture
	- ![image-20210203215606937](../../assets/img/boostcamp/image-20210203215606937.png){:.center}

**History of Convolutional Network**

![image-20210203215740181](../../assets/img/boostcamp/image-20210203215740181.png)
더 좋은 성능을 발휘하는 모델일 수록 파라미터의 수는 작아지는 방향으로 발전해왔다. 

<br>



## 1.7. DenseNet

**DenseNet uses concatenation instead of addition**

![image-20210203220135858](../../assets/img/boostcamp/image-20210203220135858.png){:width="60%"}{:.center}



- (좌측) ResNet의 경우, skip-connection을 수행할 때 `+` pairwise하게 더해준다. 
- (우측) DenseNet의 경우, skip-connection을 수행할 때 `concat`을 통해 나열해준다.
- 이 때, 단순히 concat을 반복하게 되면 layer가 깊어질수록 아래와 같이 파라미터가 중첩되고, 마지막 층에 가서는 그 수가 기하급수적으로 늘어나게 된다. 

	![image-20210203220541835](../../assets/img/boostcamp/image-20210203220541835.png)



**Dense Block**

- Each layer concatenates the feature maps of all preceding layers
- the number of channels increases geometrically.



이를 해결하기 위해 아래와 같은 Transition block을 Dense block과 반복하며 파라미터를 조절해준다. 



**Transition block**
- batchnorm -> 1x1conv -> 2x2 Avgpooling
- Dimension reduction

	![image-20210203221419645](../../assets/img/boostcamp/image-20210203221419645.png){:.width="80%"}
	

<br>​	

## 1.8. Summary

**key takeaways**

> `VGG` : repeated 3$\times$3 blocks  
> `GoogLeNet` : 1$\times$1 convolution  
> `ResNet` : skip-connection  
> `DenseNet` : concatenation  

---

<br>



#  2. What is Convolution?

CNN(Convolutional Neural Network)에서 가장 중요한 연산은 **Convolution** 입니다. CNN에 대한 공부를 하기 전에 **Convolution의 정의, convolution 연산 방법과 기능**에 대해 배웁니다. 그리고 Convolution, 입력을 축소하는 Pooling layer, 모든 노드를 연결하여 최종적인 결과를 만드는 Fully connected layer 로 구성되는 **기본적인 CNN(Convolutional Neural Network) 구조**에 대해 배웁니다. 



## 2.1. Convolution

필터의 모양에 따라서 같은 이미지가 다르게 표현될 수 있다. 

![conv](../../assets/img/boostcamp/conv.gif)
$$
\begin{align}
O_{11} = &I_{11}K_{11}+I_{12}K_{12}+I_{13}K_{13}+I_{21}K_{21}+I_{22}K_{22}+I_{23}K_{23}+I_{31}K_{31}+I_{32}K_{32}\\
&+I_{33}K_{33}+bias\\
O_{12} = &I_{12}K_{11}+I_{13}K_{12}+I_{14}K_{13}+I_{22}K_{21}+I_{23}K_{22}+I_{24}K_{23}+I_{32}K_{31}+I_{33}K_{32}\\
&+I_{33}K_{33}+bias\\
O_{13} = &I_{13}K_{11}+I_{14}K_{12}+I_{15}K_{13}+I_{23}K_{21}+I_{24}K_{22}+I_{25}K_{23}+I_{33}K_{31}+I_{34}K_{32}\\
&+I_{34}K_{33}+bias\\
O_{14} = &I_{14}K_{11}+I_{15}K_{12}+I_{16}K_{13}+I_{24}K_{21}+I_{25}K_{22}+I_{26}K_{23}+I_{34}K_{31}+I_{35}K_{32}\\
&+I_{35}K_{33}+bias\\
\end{align}
$$


<br>



## 2.2. CNN

![image-20210203225547769](../../assets/img/boostcamp/image-20210203225547769-1612361470642.png)

CNN consists of convolution layer, pooling layer, and fully connected layer.

- Convolution and pooling layers: feature extraction
- Fully connected layer : decision making (e.g., classification)

![image-20210203230804717](../../assets/img/boostcamp/image-20210203230804717-1612361470642.png)

> 필수적이진 않지만, stride와 channel을 보고 파라미터의 크기를 가늠할 수 있어야 한다.

*stride와 padding에 대한 설명은 생략하도록 한다.*

<br>



## 2.3. Convolution Arithmetic

>  Padding (1), Stride (1), $3\times 3$ Kernel

![image-20210203231923207](../../assets/img/boostcamp/image-20210203231923207.png)

결국 Input의 Width와 Height는 convolution 연산을 수행하는 과정에서 발생하는 parameter 수에 상관이 없다. 

Kernel size 3$\times$3은 사실 3$\times$3$\times$`Channel size`이므로 (그림의 파란색 부분) 파라미터는 3$\times$3$\times$128(이전 channel size)$\times$64(변한 뒤의 channel size)가 된다. 



<br>



# 3. Computer Vision Applications

Computer Vision 에서 CNN을 이용한 분야를 알아보고자 합니다. **Semantic segmentation**의 정의, 핵심 아이디어에 대해 배웁니다. **Object detection**의 정의, 핵심 아이디어, 추가적으로 종류에 대해 배웁니다. 



## 3.1. Semantic Segmentation

**문제정의** 

- 이미지의 모든 픽셀을 분류하는 문제

	![image-20210203171918154](../../assets/img/boostcamp/image-20210203171918154.png)

- Dense classification이라고도 불린다  
- 자율주행에 사용되는 핵심 기술이다. 

    ![](https://www.jeremyjordan.me/content/images/2018/05/deeplabcityscape.gif)
	
    > refer : `https://www.jeremyjordan.me/content/images/2018/05/deeplabcityscape.gif`



<br>



### 3.1.1. FCN(Fully Convolutional Network)

<br>



#### difference Dense and Fully conv.

- Dense
	- 마지막에 tensor를 flatten 한 뒤에 dense layer를 통해 벡터를 일자로 펴준다. 
- Fully conv.
	- Convolutionalization이라고도 부른다.
	- flatten과 dense layer를 없애는 것 자체가 장점이다. 

<br>

#### Convolutionalization

- Dense layer

	![image-20210203173834917](../../assets/img/boostcamp/image-20210203173834917.png){: .center}

	> number of param. : 4 X 4 X 16 X 10 = `2,560`

- Fully conv.

	![image-20210203173944524](../../assets/img/boostcamp/image-20210203173944524.png){: .center}

	> number of param. : 4 X 4 X 16 X 10 = `2,560`

- 보이는 것과 같이 파라미터의 수에는 변화가 없다. 

<br>



#### 특징

- Transforming fully connected layers into convolution layers enables a classification net to output a heat map.
- 즉, 인풋 이미지에 상관없이 네트워크가 돌아가는 것이 특징이다. 또한, output이 커지게 되면 뒷단의 네트워크가 따라서 커진다. 
	- 왜일 간격이 개같니

<br>



#### FCN

- while FCN can run with inputs of any size, the output dimensions are typically reduced by subsampling.
- So we need a way to connect the coarse output to the dense pixels

<br>



#### Deconvolution(conv transpose)



![image-20210203182624114](../../assets/img/boostcamp/image-20210203182624114.png){:width="70%"}{: .center}

- 직관적으로는 conv 연산의 `역연산`이다.
- 하지만, 역연산이라는 건 실제로 불가능하다. 예를 들면, 어떠한 두 수를 더해서 10을 만들었다고 할 때, 10을 만드는 두 개의 수는 여러가지이다. 하지만, 거꾸로 10을 만드는 두 수를 찾으라고 한다면 이는 불가능하다.
	- convolution을 진행하면서 filter의 정보를 합해 더 작은 feature로 만들기 때문에, 이 방식의 역 연산은 불가능하다.
- 하지만 비슷한 size를 원하기 때문에, 많은 padding을 부여함으로써 이와 같이 deconvoluion 연산을 정의하게 된다. 



![](https://t1.daumcdn.net/cfile/tistory/99CA8E3359FE990510)

<br>



#### Result

- 이와 같은 방식을 통해 sementic segmentation task를 진행한다. 
<br>





---
<br>

## 3.2. Detection

per pixel이 아닌 bounding box로써 이미지 중 어떠한 부분을 검출하는 분야이다. 

<br>

### 3.1. R-CNN

![image-20210203184512116](../../assets/img/boostcamp/image-20210203184512116.png)

R-CNN (1) tackes an input images, 92) extracts around 2,000 region proposals (using Selective search), (3) compute features for each proposal (using AlexNet), and then (4) classifies with linear SVMs.

<br>

### 23.2. SPPNet

RCNN의 문제는 image의 bounding box를 뽑을 때, 그 모든 patch들을 CNN에 다 통과시켜야 한다는 점이다. 

![image-20210203185044785](../../assets/img/boostcamp/image-20210203185044785.png)

> In `R-CNN`, the number of crop / warp is usually over 2,000 meaning that CNN must run more than 2,000 times (59s/image on CPU).  
> However, in `SPPNet`, CNN runs once.

이미지의 patch를 추출(crop)하고, 이를 사이즈 조절(warp)한 뒤 conv-layer를 통과시키는 R-CNN은 각 patch를 **모두** conv에 통과시켜야 했지만, SPPNet의 경우 한 이미지만을 conv에 통과시킨 후, 각 부분별 **s**patial **p**yramid **p**ooling을 진행함으로써 속도를 개선시켰다. 


<br>

### 3.3. Fast R-CNN

1. Take an input and a set of bounding boxes. (patch를 뽑는다 and region 정보 얻는다.)
2. Generated convolutional feature map(conv feature map 통과)
3. For each region, get a fixed length feature from ROI(Region of Interest) pooling(각각의 region에 대해서 fixed length feature를 뽑는다.)
4. Two outpus: class and bounding-box regressor(neural network를 통해 bounding box의 라벨을 찾는다. )

![image-20210203192439029](../../assets/img/boostcamp/image-20210203192439029.png)

![image-20210203195832312](../../assets/img/boostcamp/image-20210203195832312.png)

> reference : cs231, 2017 lecture 1

CNN을 통과한 feature값에 image의 bounding box 부분에 대한 pooling을 진행하는 방식이다. 이후 이 pooling output을 FC-layer에 넣게 된다. 

<br>

### 3.4. Faster R-CNN

Faster R-CNN = **Region Proposal Network** + Fast R-CNN

내가 bounding box를 뽑는 것도 학습을 통해서 뽑자!는 모델이다. 
<br>

#### What is RPN?

이미지 안에서 지정된 바운딩 박스에 물체가 있는지 없는지를 찾아주는 알고리즘이다. 

![image-20210203201634172](../../assets/img/boostcamp/image-20210203201634172.png){: .center}

여기서 `anchor box`는 미리 만들어 놓은 bounding box의 크기이다. 즉, 미리 템플릿을 만들어놓은 후 이 템플릿의 사이즈가 어떻게 바뀔지에 대한 학습을 진행한다. 

![image-20210203202010143](../../assets/img/boostcamp/image-20210203202010143.png)

학습을 진행하면서 Fully conv.가 물체가 해당 바운딩 박스에 포함될지 여부에 대한 정보를 가지고 있게 된다. 

- RPN의 경우에는 `predefined region size가 9개` 있다. (128, 256, 512)의 사이즈를 가지는 region이 1:1, 1:2, 2:1)의 비율로 정한 뒤 9개의 region size 중 하나를 고르게 된다. 
- 각각의 바운딩 박스에 대한 정보는 `4개의 parameter`로 결정된다. (키우고 줄일지, width or height)
- 이 파라미터는 해당 바운딩 박스가 `쓸모 있는지 없는지` 여부를 판단한다. (whether to use it or not)

<br>



### 3.5. YOLO v1

YOLO (v1) is an extremely fast object detection algorithm.

- baseline : 45fps / smaller version : 155fps

it **simultaneously** predicts multiple bounding boxes and class probabilities.

- No explicit bounding box sampling(compared with Faster R-CNN)

> 속도가 Faster R-CNN보다도 빠른 이유는 모델을 거친 뒤 나오는 것이 아니라, 딱 한장만을 보고 레이블링을 찍어내기 때문이다. You Only ~~Live~~Look Once!

![image-20210203203436584](../../assets/img/boostcamp/image-20210203203436584.png)



1. Given an image, **YOLO** divides it into S$\times$S grid
	If the center of an object falls into the grid cell, that grid cell is responsible for detection.
2. Each cell predicts B bounding boxes(B=5). 
	Each bounding box predicts
	- box refinement (X / y / w / h)
	- confidence(of objectness)
3. Each cell predicts C class probabilities. (해당 바운딩 박스의 중점에 있는 object가 어떤 클래스인지를 예측한다. )
4. In total, it becomes a tensor with S$\times$S$\times$(B*5+C) size.
	- S$\times$S : Number of cells of the grid
	- B*5 : B bounding boxes with offsets  (X / y / w / h) and confidence
	- C : Number of classes

