---
layout: post
title: Computer Vision
subtitle: CNN과 Convolution
thumbnail-img : /assets/img/boostcamp/modern.png
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [BOOSTCAMP]
tags: [boostcamp]
comments: true
---

ILSVRC라는 Visual Recognition Challenge와 대회에서 수상을 했던 **5개 Network** 들의 주요 아이디어와 구조에 대해 배웁니다. 또한, CNN(Convolutional Neural Network)에서 가장 중요한 연산은 **Convolution** 입니다. 

CNN에 대한 공부를 하기 전에 **Convolution의 정의, convolution 연산 방법과 기능**에 대해 배웁니다. 그리고 Convolution, 입력을 축소하는 Pooling layer, 모든 노드를 연결하여 최종적인 결과를 만드는 Fully connected layer 로 구성되는 **기본적인 CNN(Convolutional Neural Network) 구조**에 대해 배웁니다. 

마지막으로 Computer Vision 에서 CNN을 이용한 분야를 알아보고자 합니다. **Semantic segmentation**의 정의, 핵심 아이디어에 대해 배웁니다. **Object detection**의 정의, 핵심 아이디어, 추가적으로 종류에 대해 배웁니다. 



# 1. Modern CNN - about 1x1 convolution



**Network list**

- **AlexNet**
	- 최초로 Deep Learning을 이용하여 ILSVRC에서 수상.
- **VGGNet**
	- 3x3 Convolution을 이용하여 Receptive field는 유지하면서 더 깊은 네트워크를 구성.
	- [Receptive field 참고 자료](https://cs231n.github.io/convolutional-networks/#conv)
- **GoogLeNet**
	- Inception blocks 을 제안.
- **ResNet**
	- Residual connection(Skip connection)이라는 구조를 제안.
	- h(x) = f(x) + x 의 구조
- **DenseNet**
	- Resnet과 비슷한 아이디어지만 Addition이 아닌 Concatenation을 적용한 CNN.

 

## 1.1. ILSVR

Imagenet Large-Scale Visual Recognition Challenge
- classification / detection / localization / segmentation
- 1000diff categories
- 1 million images

	| year  | Error Rate |
	| :---: | :--------: |
	| 2010  |    28%     |
	| 2011  |    25%     |
	| 2012  |    16%     |
	| 2013  |    11%     |
	| 2104  |     6%     |
	| 2015  |    3.5%    |
	| Human | about 5.1% |



## 1.2. AlexNet

**key ideas**

- ReLU activation
- 2 GPUs
- local response normalization, overlapping pooling
- data augmentation
- dropout

**relu activation** 

- preserves properties of linear model
- easy to optimize with gradient descent
- good generalization
- overcome the vanishing gradient problem



<br>



## 1.3. VGGNet

![image-20210203213902867](https://user-images.githubusercontent.com/38639633/106762216-f6f08a80-6678-11eb-81d2-96a2ed2761c8.png){: .center}

**key ideas**

- 3 X 3 convolution. filter의 사이즈를 줄이기 시작했다. 
- why? 

  ![image-20210203213624646](https://user-images.githubusercontent.com/38639633/106762214-f657f400-6678-11eb-89d1-4a201303b1ca.png){: width = "70%"}

  - 3 by 3 filter를 두 번 사용하는 것은 5 by 5 filter를 한 번 사용하는 것과 I/O의 관점에서 별 차이가 없다. 
  - 하지만 파라미터의 수를 계산해보면 약 10만개 가량이 차이난다. 
  - 이는 계산속도와 generalization performance에 영향을 끼친다.

<br>



## 1.4. Googlenet

![image-20210203214007157](https://user-images.githubusercontent.com/38639633/106762218-f7892100-6678-11eb-9f60-338298202cd4.png){: .center}

**inception blocks**

- what are the **benefits** of the `inception block`?
	
	- reduce the number of parameter
	
- How?
  - recall how the number of parameters is computed
  - 1 X 1 convolution can be seen as channel wise dimension reduction, 
    
      > 즉 채널 방향으로 차원을 줄일 수 있다. 

  ![image-20210203214117290](https://user-images.githubusercontent.com/38639633/106762220-f7892100-6678-11eb-8cfa-ef404fec7876.png){:.center}

  - 앞서 설명한 "작은 사이즈의 filter를 여러번 사용하는 방법"과 유사하게,   
  	layer는 깊게 만들고 I/O size는 유지하면서 파라미터의 사이즈는 줄일 수 있다. 
  - 약 30% 가량으로 파라미터가 줄어든 모습을 확인할 수 있다. 
  
  

**benefit of 1,1 conv** 
- 1x1 convolution enables about 30% reduce of the number of pararmeters

<br>

## 1.5. comparing

**\# of parameters?**

- `AlexNet(8-layers)` : 60M
- `VGGNet(19-layers)` : 110M
- `GoogLeNet(22-layers)` : 4M

<br>

## 1.6. ResNet

**deeper neural networks are hard to train**

- overfitting is usually caused by an excessive number of parameters
- bus not in this case.

**idea**

- add an identity map(skip connection)

  

  ![image-20210203215048189](https://user-images.githubusercontent.com/38639633/106762221-f821b780-6678-11eb-99fb-dee6840a86ce.png){:.center}
  ![image-20210203215118424](https://user-images.githubusercontent.com/38639633/106762223-f821b780-6678-11eb-8d88-362a409eca27.png){:.center}



- skip-connection을 사용하지 않을 때(**좌측**)는 더 깊은 신경망의 error가 더 큰 모습을 보여준다. 하지만,  ResNet의 경우(**우측**), 신경망의 깊이가 깊어졌음에도 불구하고 얕은 신경망에 비해서 더 낮은 error율을 보여준다.   
	이는 신경망이 깊어졌음에도 학습을 제대로 하고 있다는 것을 보여준다. 
- add an identity map after nonlinear activations:
	- 차원을 맞춰주기 위해 1x1 conv를 이용한다. 
- bottleneck architecture
	- ![image-20210203215606937](https://user-images.githubusercontent.com/38639633/106762224-f8ba4e00-6678-11eb-9cca-935e449e4719.png){:.center}

**History of Convolutional Network**

![image-20210203215740181](https://user-images.githubusercontent.com/38639633/106762227-f8ba4e00-6678-11eb-80b5-787b53b10d3b.png)
더 좋은 성능을 발휘하는 모델일 수록 파라미터의 수는 작아지는 방향으로 발전해왔다. 

<br>



## 1.7. DenseNet

**DenseNet uses concatenation instead of addition**

![image-20210203220135858](https://user-images.githubusercontent.com/38639633/106762230-f952e480-6678-11eb-98da-84c1797f9e96.png){:width="60%"}{:.center}



- (좌측) ResNet의 경우, skip-connection을 수행할 때 `+` pairwise하게 더해준다. 
- (우측) DenseNet의 경우, skip-connection을 수행할 때 `concat`을 통해 나열해준다.
- 이 때, 단순히 concat을 반복하게 되면 layer가 깊어질수록 아래와 같이 파라미터가 중첩되고, 마지막 층에 가서는 그 수가 기하급수적으로 늘어나게 된다. 

	![image-20210203220541835](https://user-images.githubusercontent.com/38639633/106762232-f952e480-6678-11eb-88eb-f74fc85a91f9.png)



**Dense Block**

- Each layer concatenates the feature maps of all preceding layers
- the number of channels increases geometrically.



이를 해결하기 위해 아래와 같은 Transition block을 Dense block과 반복하며 파라미터를 조절해준다. 



**Transition block**
- batchnorm -> 1x1conv -> 2x2 Avgpooling
- Dimension reduction

	![image-20210203221419645](https://user-images.githubusercontent.com/38639633/106762235-f9eb7b00-6678-11eb-9aa5-abc630726ec7.png){:.width="80%"}
	

<br>​	

## 1.8. Summary

**key takeaways**

> `VGG` : repeated 3$\times$3 blocks  
> `GoogLeNet` : 1$\times$1 convolution  
> `ResNet` : skip-connection  
> `DenseNet` : concatenation  

---

<br>



#  2. What is Convolution?

CNN(Convolutional Neural Network)에서 가장 중요한 연산은 **Convolution** 입니다. CNN에 대한 공부를 하기 전에 **Convolution의 정의, convolution 연산 방법과 기능**에 대해 배웁니다. 그리고 Convolution, 입력을 축소하는 Pooling layer, 모든 노드를 연결하여 최종적인 결과를 만드는 Fully connected layer 로 구성되는 **기본적인 CNN(Convolutional Neural Network) 구조**에 대해 배웁니다. 



## 2.1. Convolution

필터의 모양에 따라서 같은 이미지가 다르게 표현될 수 있다. 

![conv](../../assets/img/boostcamp/conv.gif)
$$
\begin{align}
O_{11} = &I_{11}K_{11}+I_{12}K_{12}+I_{13}K_{13}+I_{21}K_{21}+I_{22}K_{22}+I_{23}K_{23}+I_{31}K_{31}+I_{32}K_{32}\\
&+I_{33}K_{33}+bias\\
O_{12} = &I_{12}K_{11}+I_{13}K_{12}+I_{14}K_{13}+I_{22}K_{21}+I_{23}K_{22}+I_{24}K_{23}+I_{32}K_{31}+I_{33}K_{32}\\
&+I_{33}K_{33}+bias\\
O_{13} = &I_{13}K_{11}+I_{14}K_{12}+I_{15}K_{13}+I_{23}K_{21}+I_{24}K_{22}+I_{25}K_{23}+I_{33}K_{31}+I_{34}K_{32}\\
&+I_{34}K_{33}+bias\\
O_{14} = &I_{14}K_{11}+I_{15}K_{12}+I_{16}K_{13}+I_{24}K_{21}+I_{25}K_{22}+I_{26}K_{23}+I_{34}K_{31}+I_{35}K_{32}\\
&+I_{35}K_{33}+bias\\
\end{align}
$$


<br>



## 2.2. CNN

![image-20210203225547769-1612361470642](https://user-images.githubusercontent.com/38639633/106762608-551d6d80-6679-11eb-9061-78268d815547.png)

CNN consists of convolution layer, pooling layer, and fully connected layer.

- Convolution and pooling layers: feature extraction
- Fully connected layer : decision making (e.g., classification)

![image-20210203231020465](https://user-images.githubusercontent.com/38639633/106762612-55b60400-6679-11eb-9a8b-61d9f7e47d85.png)

> 필수적이진 않지만, stride와 channel을 보고 파라미터의 크기를 가늠할 수 있어야 한다.

*stride와 padding에 대한 설명은 생략하도록 한다.*

<br>



## 2.3. Convolution Arithmetic

>  Padding (1), Stride (1), $3\times 3$ Kernel

![image-20210203231923207](https://user-images.githubusercontent.com/38639633/106762614-564e9a80-6679-11eb-9db5-c48a46fc27d9.png)

결국 Input의 Width와 Height는 convolution 연산을 수행하는 과정에서 발생하는 parameter 수에 상관이 없다. 

Kernel size 3$\times$3은 사실 3$\times$3$\times$`Channel size`이므로 (그림의 파란색 부분) 파라미터는 3$\times$3$\times$128(이전 channel size)$\times$64(변한 뒤의 channel size)가 된다. 



<br>



# 3. Computer Vision Applications

Computer Vision 에서 CNN을 이용한 분야를 알아보고자 합니다. **Semantic segmentation**의 정의, 핵심 아이디어에 대해 배웁니다. **Object detection**의 정의, 핵심 아이디어, 추가적으로 종류에 대해 배웁니다. 



## 3.1. Semantic Segmentation

**문제정의** 

- 이미지의 모든 픽셀을 분류하는 문제

	![image-20210203171918154](https://user-images.githubusercontent.com/38639633/106762834-901fa100-6679-11eb-8c43-c7f510230d06.png)

- Dense classification이라고도 불린다  
- 자율주행에 사용되는 핵심 기술이다. 

    ![](https://www.jeremyjordan.me/content/images/2018/05/deeplabcityscape.gif)
	
    > refer : `https://www.jeremyjordan.me/content/images/2018/05/deeplabcityscape.gif`



<br>



### 3.1.1. FCN(Fully Convolutional Network)

<br>



#### difference Dense and Fully conv.

- Dense
	- 마지막에 tensor를 flatten 한 뒤에 dense layer를 통해 벡터를 일자로 펴준다. 
- Fully conv.
	- Convolutionalization이라고도 부른다.
	- flatten과 dense layer를 없애는 것 자체가 장점이다. 

<br>

#### Convolutionalization

- Dense layer

	![image-20210203173834917](https://user-images.githubusercontent.com/38639633/106762837-90b83780-6679-11eb-9039-3666aeb6f411.png){: .center}

	> number of param. : 4 X 4 X 16 X 10 = `2,560`

- Fully conv.

	![image-20210203173944524](https://user-images.githubusercontent.com/38639633/106762838-9150ce00-6679-11eb-933a-2327680c6422.png){: .center}

	> number of param. : 4 X 4 X 16 X 10 = `2,560`

- 보이는 것과 같이 파라미터의 수에는 변화가 없다. 

<br>



#### 특징

- Transforming fully connected layers into convolution layers enables a classification net to output a heat map.
- 즉, 인풋 이미지에 상관없이 네트워크가 돌아가는 것이 특징이다. 또한, output이 커지게 되면 뒷단의 네트워크가 따라서 커진다. 
	- 왜일 간격이 개같니

<br>



#### FCN

- while FCN can run with inputs of any size, the output dimensions are typically reduced by subsampling.
- So we need a way to connect the coarse output to the dense pixels

<br>



#### Deconvolution(conv transpose)



![image-20210203182624114](https://user-images.githubusercontent.com/38639633/106762839-9150ce00-6679-11eb-961c-1d67a4de0582.png){:width="70%"}{: .center}

- 직관적으로는 conv 연산의 `역연산`이다.
- 하지만, 역연산이라는 건 실제로 불가능하다. 예를 들면, 어떠한 두 수를 더해서 10을 만들었다고 할 때, 10을 만드는 두 개의 수는 여러가지이다. 하지만, 거꾸로 10을 만드는 두 수를 찾으라고 한다면 이는 불가능하다.
	- convolution을 진행하면서 filter의 정보를 합해 더 작은 feature로 만들기 때문에, 이 방식의 역 연산은 불가능하다.
- 하지만 비슷한 size를 원하기 때문에, 많은 padding을 부여함으로써 이와 같이 deconvoluion 연산을 정의하게 된다. 



![](https://t1.daumcdn.net/cfile/tistory/99CA8E3359FE990510)

<br>



#### Result

- 이와 같은 방식을 통해 sementic segmentation task를 진행한다. 
<br>





---
<br>

## 3.2. Detection

per pixel이 아닌 bounding box로써 이미지 중 어떠한 부분을 검출하는 분야이다. 

<br>

### 3.1. R-CNN

![image-20210203184512116](https://user-images.githubusercontent.com/38639633/106762840-91e96480-6679-11eb-9133-973e27511b42.png)

R-CNN (1) tackes an input images, 92) extracts around 2,000 region proposals (using Selective search), (3) compute features for each proposal (using AlexNet), and then (4) classifies with linear SVMs.

<br>

### 23.2. SPPNet

RCNN의 문제는 image의 bounding box를 뽑을 때, 그 모든 patch들을 CNN에 다 통과시켜야 한다는 점이다. 

![image-20210203185044785](https://user-images.githubusercontent.com/38639633/106762842-91e96480-6679-11eb-89ce-2573b87e4665.png)

> In `R-CNN`, the number of crop / warp is usually over 2,000 meaning that CNN must run more than 2,000 times (59s/image on CPU).  
> However, in `SPPNet`, CNN runs once.

이미지의 patch를 추출(crop)하고, 이를 사이즈 조절(warp)한 뒤 conv-layer를 통과시키는 R-CNN은 각 patch를 **모두** conv에 통과시켜야 했지만, SPPNet의 경우 한 이미지만을 conv에 통과시킨 후, 각 부분별 **s**patial **p**yramid **p**ooling을 진행함으로써 속도를 개선시켰다. 


<br>

### 3.3. Fast R-CNN

1. Take an input and a set of bounding boxes. (patch를 뽑는다 and region 정보 얻는다.)
2. Generated convolutional feature map(conv feature map 통과)
3. For each region, get a fixed length feature from ROI(Region of Interest) pooling(각각의 region에 대해서 fixed length feature를 뽑는다.)
4. Two outpus: class and bounding-box regressor(neural network를 통해 bounding box의 라벨을 찾는다. )

![image-20210203192513937](https://user-images.githubusercontent.com/38639633/106762843-9281fb00-6679-11eb-9171-f78c46e20bd8.png)

![image-20210203195832312](https://user-images.githubusercontent.com/38639633/106762845-931a9180-6679-11eb-8087-05fd9d9cd685.png)

> reference : cs231, 2017 lecture 1

CNN을 통과한 feature값에 image의 bounding box 부분에 대한 pooling을 진행하는 방식이다. 이후 이 pooling output을 FC-layer에 넣게 된다. 

<br>

### 3.4. Faster R-CNN

Faster R-CNN = **Region Proposal Network** + Fast R-CNN

내가 bounding box를 뽑는 것도 학습을 통해서 뽑자!는 모델이다. 
<br>

#### What is RPN?

이미지 안에서 지정된 바운딩 박스에 물체가 있는지 없는지를 찾아주는 알고리즘이다. 

![image-20210203201634172](https://user-images.githubusercontent.com/38639633/106762846-931a9180-6679-11eb-87d2-10bd0d0834b5.png){: .center}

여기서 `anchor box`는 미리 만들어 놓은 bounding box의 크기이다. 즉, 미리 템플릿을 만들어놓은 후 이 템플릿의 사이즈가 어떻게 바뀔지에 대한 학습을 진행한다. 

![image-20210203202010143](https://user-images.githubusercontent.com/38639633/106762850-93b32800-6679-11eb-9a2c-ac6eb34db561.png)

학습을 진행하면서 Fully conv.가 물체가 해당 바운딩 박스에 포함될지 여부에 대한 정보를 가지고 있게 된다. 

- RPN의 경우에는 `predefined region size가 9개` 있다. (128, 256, 512)의 사이즈를 가지는 region이 1:1, 1:2, 2:1)의 비율로 정한 뒤 9개의 region size 중 하나를 고르게 된다. 
- 각각의 바운딩 박스에 대한 정보는 `4개의 parameter`로 결정된다. (키우고 줄일지, width or height)
- 이 파라미터는 해당 바운딩 박스가 `쓸모 있는지 없는지` 여부를 판단한다. (whether to use it or not)

<br>



### 3.5. YOLO v1

YOLO (v1) is an extremely fast object detection algorithm.

- baseline : 45fps / smaller version : 155fps

it **simultaneously** predicts multiple bounding boxes and class probabilities.

- No explicit bounding box sampling(compared with Faster R-CNN)

> 속도가 Faster R-CNN보다도 빠른 이유는 모델을 거친 뒤 나오는 것이 아니라, 딱 한장만을 보고 레이블링을 찍어내기 때문이다. You Only ~~Live~~Look Once!

![image-20210203203436584](https://user-images.githubusercontent.com/38639633/106762851-944bbe80-6679-11eb-8915-1ee715d2df40.png)



1. Given an image, **YOLO** divides it into S$\times$S grid
	If the center of an object falls into the grid cell, that grid cell is responsible for detection.
2. Each cell predicts B bounding boxes(B=5). 
	Each bounding box predicts
	- box refinement (X / y / w / h)
	- confidence(of objectness)
3. Each cell predicts C class probabilities. (해당 바운딩 박스의 중점에 있는 object가 어떤 클래스인지를 예측한다. )
4. In total, it becomes a tensor with S$\times$S$\times$(B*5+C) size.
	- S$\times$S : Number of cells of the grid
	- B*5 : B bounding boxes with offsets  (X / y / w / h) and confidence
	- C : Number of classes

