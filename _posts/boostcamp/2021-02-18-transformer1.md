---
layout: post
title: ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech - Transformer I
subtitle: ê·¸ì•¼ë§ë¡œ Attention is all you need
thumbnail-img : https://user-images.githubusercontent.com/38639633/108290231-5307f280-71d3-11eb-9576-f3cf9eca37a0.png
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [BOOSTCAMP]
tags: [boostcamp,transformer, machine translation]
comments: true
---

ì´ë²ˆ ê°•ì˜ì—ì„œëŠ” í˜„ì¬ NLP ì—°êµ¬ ë¶„ì•¼ì—ì„œ ê°€ì¥ ë§ì´ í™œìš©ë˜ê³  ìˆëŠ” Transformer(Self-Attention)ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë´…ë‹ˆë‹¤. Self-Attentionì€ RNN ê¸°ë°˜ ë²ˆì—­ ëª¨ë¸ì˜ ë‹¨ì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì²˜ìŒ ë“±ì¥í–ˆìŠµë‹ˆë‹¤. RNNê³¼ Attentionì„ í•¨ê»˜ ì‚¬ìš©í–ˆë˜ ê¸°ì¡´ê³¼ëŠ” ë‹¬ë¦¬ Attention ì—°ì‚°ë§Œì„ ì´ìš©í•´ ì…ë ¥ ë¬¸ì¥/ë‹¨ì–´ì˜ representationì„ í•™ìŠµì„ í•˜ë©° ì¢€ ë” parallelí•œ ì—°ì‚°ì´ ê°€ëŠ¥í•œ ë™ì‹œì— í•™ìŠµ ì†ë„ê°€ ë¹ ë¥´ë‹¤ëŠ” ì¥ì ì„ ë³´ì˜€ìŠµë‹ˆë‹¤

**Further Reading**

- [Attention is all you need, NeurIPS'17](https://arxiv.org/abs/1706.03762)
- [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)



# Transformer I

> ê³¼ê±° Attention is all you need ë…¼ë¬¸ì„ [í¬ìŠ¤íŒ…](https://ydy8989.github.io/2021-01-10-transformer/)í–ˆë˜ ì ì´ ìˆì§€ë§Œ, naver boostcamp ê³¼ì •ì„ ìˆ˜ê°•í•˜ë©´ì„œ ë‹¤ì‹œ í•œ ë²ˆ ë“±ì¥í•œ transformerì— ëŒ€í•´ í¬ìŠ¤íŒ… í•˜ë ¤ê³  í•œë‹¤. ì§€ë‚œë²ˆì—” ë…¼ë¬¸ì˜ íë¦„ì— ë”°ë¼ ì„¤ëª…ì„ ì§„í–‰í–ˆë‹¤ë©´ ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” ì¡°ê¸ˆ ë” ì‹¤ì§ˆì ì´ê³  ì‚¬ìš©ì  ì¸¡ë©´ì—ì„œ ë°”ë¼ë³´ë©° í¬ìŠ¤íŒ…í•  ì˜ˆì •ì´ë‹¤. 



## RNN: Long-term dependency

![image](https://user-images.githubusercontent.com/38639633/108290420-b4c85c80-71d3-11eb-8d2d-dcbe3e1a4d69.png)

- "I go home"ì´ë¼ëŠ” ë¬¸ì¥ì„ ë°›ì•˜ì„ ë•Œ ë§¤ time step $t$ë§ˆë‹¤ $x_t, h_{t-1}$ì„ ë°›ì•„ì„œ $h_t$ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤. 
- ê·¸ë¦¼ì—ì„œ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ ë°©í–¥ìœ¼ë¡œ ê°€ë©° ê³„ì‚°ë˜ëŠ” hidden stateë¥¼ encodingí•˜ê²Œ ëœë‹¤. 
- Attention ì—°ì‚°ì„ í•œë‹¤í•´ë„, ë’¤ë¡œ ê°ˆìˆ˜ë¡ ë¨¼ì € ì…ë ¥ëœ ë‹¨ì–´ "I"ëŠ” í¬ì„ë˜ê²Œ ëœë‹¤.  



## Bi-Directional RNNs

![image](https://user-images.githubusercontent.com/38639633/108291960-81d39800-71d6-11eb-945c-96fb9f0bd052.png)

- Vanilla RNNì˜ ë‹¨ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•œ ë°©ì‹ìœ¼ë¡œ ì œì•ˆëœ Bi-directional RNNì€ ì—­ë°©í–¥ìœ¼ë¡œë„ í•œë²ˆ ë” ì§„í–‰í•´ì˜¤ë©´ì„œ ì–‘ë°©í–¥ì—ì„œì˜ encoding ë²¡í„°ë¥¼ í•™ìŠµí•œë‹¤. 
- ì–‘ë°©í–¥ìœ¼ë¡œ ì§„í–‰ë˜ëŠ” Forward RNNê³¼ Backward RNN ëª¨ë“ˆì„ ë³‘ë ¬ì ìœ¼ë¡œ ë§Œë“¤ê³  íŠ¹ì •í•œ timestepì—ì„œì˜ hidden state vectorë¥¼ concatenateí•¨ìœ¼ë¡œì¨ ë‘ ë°°ì˜ ì‚¬ì´ì¦ˆë¡œ ë§Œë“¤ì–´ì§„ encoding vectorë¥¼ ë§Œë“ ë‹¤. 



## Transformer: Long-Term Dependency

![image](https://user-images.githubusercontent.com/38639633/108292428-6b7a0c00-71d7-11eb-80d8-66673d3e3cc7.png){:width="60%"}{:.center}

- Transformerì˜ attention ì—°ì‚°ì€ self-attentionìœ¼ë¡œì¨, ê¸°ì¡´ attentionì—ì„œ encoderì™€ decoderì˜ ì…ë ¥ ë²¡í„°ê°€ ë‹¬ëë˜ ê²ƒê³¼ ë‹¬ë¦¬ ê°™ì€ hidden state vectorë¥¼ ì‚¬ìš©í•œë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤. 
- ì¦‰, ê·¸ë¦¼ì—ì„œ $x_1$ì€ decoder hidden state vectorì„ê³¼ ë™ì‹œì— encoder hidden state vector setì¸ $[x_1, x_2, x_3]$ì¤‘ í•˜ë‚˜ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. 
- ê·¸ëŸ¬ë©´ ì²« ë²ˆì§¸ timestepì„ ê¸°ì¤€ìœ¼ë¡œ $x_1$ì€ $[x_1, x_2, x_3]$ ì„¸ encoder hidden states ë“¤ê³¼ ë‚´ì ì„ í†µí•´ attention scoreë¥¼ ê³„ì‚°í•˜ê²Œ ë˜ê³ , ì´ëŠ” $h_1$ìœ¼ë¡œ ê³„ì‚°ë  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. 
- ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ $h_2, h_3$ë¥¼ êµ¬í•˜ê²Œ ë˜ëŠ” í° í‹€ì—ì„œì˜ ë°©ì‹ì„ `Self-Attention`ì´ë¼ê³  ë¶€ë¥¸ë‹¤.
- í•˜ì§€ë§Œ, ì¼ë°˜ì ì¸ ë°©ì‹ìœ¼ë¡œ ê³„ì‚°í•˜ê²Œ ëœë‹¤ë©´ ë‹¹ì—°í•˜ê²Œë„ **ìê¸° ìì‹ ê³¼ì˜ ë‚´ì **ì´ í° ë¹„ì¤‘ìœ¼ë¡œ í• ë‹¹ë˜ê²Œ ë˜ê³ , self-attention moduleì˜ outputì¸ $h_{1,2,3}$ëŠ” ìê¸° ìì‹ ì— ëŒ€í•œ ê°€ì¤‘ í‰ê· ì´ ë†’ê²Œ ì¡íˆê²Œ ë  ê²ƒì´ë‹¤. 
- ë”°ë¼ì„œ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ê°œì„ í•˜ê³ ì Transformerì—ì„œëŠ” í™•ì¥ëœ ë°©ì‹ì˜ attention moduleì„ ì‚¬ìš©í•œë‹¤.



### Query, Key, Value Vectors

- **`Query vector`** : encoder-decoder êµ¬ì¡°ì—ì„œ decoder hidden state vectorì— í•´ë‹¹í•˜ëŠ” vectorë¥¼ ì˜ë¯¸í•œë‹¤. ì¦‰, í˜„ì¬ timestep $t$ì—ì„œ ê³„ì‚°í•  ì£¼ì²´ê°€ ë˜ëŠ” vector.
- **`Key vector`** : query vectorì™€ ë‚´ì ì„ í•˜ê²Œ ë  ê°ê°ì˜ ì¬ë£Œ ë²¡í„°ë¥¼ ì˜ë¯¸í•œë‹¤. ì¦‰, encoder-decoder êµ¬ì¡°ì—ì„œ encoder hidden statesì¸ $h_{1,2,3}^{(e)}$ë¥¼ ì˜ë¯¸í•œë‹¤. 
- **`Value vector`** : ê³„ì‚°ëœ ê°€ì¤‘ì¹˜(attention score)ë¥¼ ê°€ì¤‘ í‰ê· í•´ì„œ ê·¸ ë¹„ì¤‘ì„ ê°€ì¤‘í•´ì£¼ê¸° ìœ„í•´ ê³±í•´ì£¼ëŠ” ì›ë˜ ë²¡í„° 

![image](https://user-images.githubusercontent.com/38639633/108337579-9afd3880-7218-11eb-8130-b582c472370e.png)

> - $q_1\cdot k_1$, $q_1\cdot k_2$, $q_1\cdot k_3$ë¥¼ í†µí•´ [3.8, -0.2, 5.9]ì˜ vectorë¥¼ ì–»ê²Œëœë‹¤.   
> - ì´ëŠ” softmaxë¥¼ í†µê³¼í•˜ì—¬ [0.2, 0.1, 0.7]ì´ ëœë‹¤.   
> - ì´ë ‡ê²Œ ë‚˜ì˜¨ ê²°ê³¼ëŠ” [$v_1, v_2, v_3$]ê³¼ pairwise product ì—°ì‚°ì„ ì§„í–‰í•˜ê²Œëœë‹¤.   
> - ê²°ê³¼ì ìœ¼ë¡œ $h_1=0.2v_1+0.1v_2+0.7v_3$ê°€ ëœë‹¤. 

ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ì—°ì‚°ë˜ê¸° ë•Œë¬¸ì—, ìê¸° ìì‹ ì— ëŒ€í•œ self-attention ì—°ì‚°ì„ í•˜ì—¬ë„ ê·¸ í¬ê¸°ê°€ ë†’ì§€ ì•Šê²Œ ëœë‹¤. 

### Operation process in self-attention

![image](https://user-images.githubusercontent.com/38639633/108339298-9afe3800-721a-11eb-90e5-31f24e7d278f.png){:width="80%"}{:.center}

- ì‹¤ì œ ì‘ë™ì€ ìœ„ì™€ ê°™ì€ í–‰ë ¬ ì—°ì‚°ì— ì˜í•´ì„œ ì§„í–‰ëœë‹¤. 
- Embeddingëœ input $X$ëŠ” $W^{Q,K,V}$ì™€ì˜ í–‰ë ¬ê³±ì„ í†µí•´ $Q,K,V$ë¡œ êµ¬ì„±ëœë‹¤. 
- $Q,K,V$ì˜ ê° í–‰ì€ $X$ì˜ ê° í–‰, ì¦‰ ê° í† í°ì— í•´ë‹¹í•˜ëŠ” vectorê°€ ëœë‹¤. 

ì´ ê°™ì€ ë°©ì‹ì„ í†µí•´ ë¨¼ ë‹¨ì–´ê°„ì˜ ê´€ê³„ ë° ìœ ì‚¬ë„ë¥¼ ì´ì „ ëª¨ë¸ê³¼ëŠ” ë‹¬ë¦¬ ì†ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. 



## Transformer: Scaled Dot-Product Attention

- **Inputs** : a query $q$ and a set of key-value $(k, v)$ pairs to an output  
- Query, key, value, and output is all vectors  
- **Output** is weighted sum of values  
- Weight of each value is computed by an inner product of query and corresponding key  
- Queries and keys have same dimensionality $d_k$, and dimensionality of value is $d_v$
	- Value vectorëŠ” ë§ˆì§€ë§‰ì— ê³„ì‚°ëœ ê°€ì¤‘í‰ê· ì„ ê³±í•˜ëŠ” ì—­í• ë§Œì„ í•˜ê¸° ë•Œë¬¸ì— ì°¨ì›ì˜ í¬ê¸°ê°€ Query, Key vectorë“¤ê³¼ëŠ” ë‹¬ë¼ë„ ìƒê´€ì´ ì—†ë‹¤. 

$$
A(q, K, V)=\sum_i\frac{exp(q\cdot k_i)}{\sum_j exp(q\cdot k_j)}v_i
$$

- ì¦‰, inputì€ í•˜ë‚˜ì§œë¦¬ query ë²¡í„° $q$ì™€ $K, V$ê°€ ëœë‹¤.   

- ans it becomes : $A(Q, K, V) = softmax(QK^T)V$.

	![image](https://user-images.githubusercontent.com/38639633/108349953-f1717380-7226-11eb-95b1-544cc34ed8c0.png)

	> ë…¼ë¬¸ì—ì„œì˜ Transformer êµ¬í˜„ ìƒìœ¼ë¡œëŠ” ë™ì¼í•œ shapeìœ¼ë¡œ mappingëœ Q, K, Vê°€ ì‚¬ìš©ë˜ì–´ ê° matrixì˜ shapeì€ ëª¨ë‘ ë™ì¼í•˜ë‹¤. 



### Problem

- As $d_k$ gets large, the variance of $q^Tk$ increases
	- queryì™€ key vectorì˜ ì°¨ì›ì´ ì»¤ì§ˆìˆ˜ë¡ í•´ë‹¹ ë‚´ì ì— ì°¸ì—¬í•˜ëŠ” dimension ì—­ì‹œ ì»¤ì§€ê²Œ ë˜ê³  ì´ë•Œì˜ ë¶„ì‚°ì€ ì ì  ì»¤ì§€ê²Œ ëœë‹¤. 
- Some values inside the softmax get large
- The softmax gets very peaked
- Hence, its gradient gets smaller

### Solution

- Scaled by the length of query / key vectors:
	- $$A(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$
	- $\sqrt{d_k}$ë¡œ ë‚˜ëˆ ì¤Œìœ¼ë¡œì¨ scalingì„ ì‹œì¼œì¤€ë‹¤. 

![image](https://user-images.githubusercontent.com/38639633/108353903-07356780-722c-11eb-9926-69f1500536ac.png){:width="30%"}{:.center}

# Transformer II (contâ€™d)

Transformer(Self-Attention)ì— ëŒ€í•´ ì´ì–´ì„œ ìì„¸íˆ ì•Œì•„ë´…ë‹ˆë‹¤.

**Further Reading**

- [Attention is all you need, NeurIPS'17](https://arxiv.org/abs/1706.03762)
- [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [Group Normalization](https://openaccess.thecvf.com/content_ECCV_2018/papers/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.pdf)

**Further Question**

- Attentionì€ ì´ë¦„ ê·¸ëŒ€ë¡œ ì–´ë–¤ ë‹¨ì–´ì˜ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ê°€ì ¸ì˜¬ ì§€ ì•Œë ¤ì£¼ëŠ” ì§ê´€ì ì¸ ë°©ë²•ì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤. Attentionì„ ëª¨ë¸ì˜ Outputì„ ì„¤ëª…í•˜ëŠ” ë°ì— í™œìš©í•  ìˆ˜ ìˆì„ê¹Œìš”?
	- ì°¸ê³ : [Attention is not explanation](https://arxiv.org/pdf/1902.10186.pdf)
	- ì°¸ê³ : [Attention is not not explanation](https://www.aclweb.org/anthology/D19-1002.pdf)

## Transformer: Multi-Head Attention

- The input word vectors are the queries, keys and values
- In other words, the word vectors themselves select each other
- **Problem** of single attention
	- Only one way for words to interact with one another
- **Solution**
	- Multi-head attention maps $ğ‘„, ğ¾, ğ‘‰$ into the $â„$ number of lower-dimensional spaces via $ğ‘Š$ matrices
- Then apply attention, then concatenate outputs and pipe through linear layer

