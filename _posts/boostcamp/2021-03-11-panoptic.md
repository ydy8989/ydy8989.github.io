---
layout: post
title: CV / Instance, Panoptic segmentation
subtitle: Adios object detection...
thumbnail-img: https://user-images.githubusercontent.com/38639633/110747931-6530fa00-8282-11eb-838c-c4952c98f019.png
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [BOOSTCAMP]
tags: [boostcamp]
toc: true
comments: true
---



단순히 픽셀 마다의 클래스를 분류하는 semantic segmentation은 **동일한 클래스에 속하는 개별 물체**를 구분하지 못합니다. 이와 달리 instance segmentation은 영상 내에 동일한 물체가 여러 개 존재하는 경우에 각각의 물체를 구분하며 동시에 픽셀 단위의 mask도 예측하는 방법입니다. 그리고 semantic segmentation과 instance segmentation을 결합하여 더욱 복잡한 task인 panoptic segmentation을 소개합니다.

또 다른 물체를 인식하는 방법에는 각 물체를 대표하는 점들을 예측하는 것이 있습니다. 이러한 task를 landmark localization이라고 하며 사람의 동작을 인식하는 human pose estimation에 주로 사용되고 있습니다. Landmark localization을 대표하는 모델인 hourglass를 위주로 해당 task를 소개합니다.

<br> 

# Instance segmentation

## What is instance segmentation?

기존 픽셀을 같은 클래스로만 분류하는 Semantic segmentation에서 더욱 발전한 모델인 instance segmentation을 알아보자. 

`Instance segmentation`은 말 그대로 개체간의 분류도 이뤄지는 segmentation이라고 할 수 있다. 

![image-20210311153416903](https://user-images.githubusercontent.com/38639633/111062155-96abfe80-84ea-11eb-88ad-c28d7c5585ce.png){:.center}

- (좌측) 이미지를 segmentation 할 때, 
- (가운데) 이미지는 원본 이미지를 단순 semantic segmentation한 결과이다. 
- (우측) 이미지는 여기서 한발짝 더 나아가, 의자간의 분류, 그러니깐 instance까지도 분류하는 모습이다. 

<br>

## Instance segmenters

그렇다면 instance segmentation을 가능케하는 다양한 모델에 대하여 살펴보자.

<br>

### Mask R-CNN

![image](https://user-images.githubusercontent.com/38639633/111031850-f2c04580-844c-11eb-9978-34c61bcc7095.png){:.center}

- 기존 좋은 성능을 냈던, Faster R-CNN에서 `RoIAlign`이라는 새로운 pooling 방식을 추가하였다. 

	![image](https://user-images.githubusercontent.com/38639633/111031742-5b5af280-844c-11eb-9758-54baef1c37f6.png){:.center}

- Faster R-CNN은 RoI Pooling이라는 방법을 사용하여 정수 좌표에서만 feature를 뽑아왔었다. 즉, 정수가 아닌 부분에서 feature는 근사하여 뽑혔다. 
- RoIAlign은 interpolation을 통해 소수점에서도 pooling을 지원하게 된다. 이를 통해 성능improvement로 이어지게된다. 
- 또 기존의 Faster R-CNN이 마지막에 Class와 box regression branch를 사용했던 것과는 달리, 추가적으로 `mask branch`를 도입하였다. 
	![image](https://user-images.githubusercontent.com/38639633/111031752-6f065900-844c-11eb-9d3a-ac38233b7df7.png){:.center}

- 이 mask brach를 통해 binary 마스크를 classification하도록 하는 구조를 갖는다. 
- 위 그림에서는 총 80개의 클래스를 고려하도록 한다. (80개에 대하여 기인지 아닌지)
- 일괄적으로 모든 마스크를 생성하고, 다른 branch인 class head에서 나온 결과를 이용해서 어떤 마스크를 사용할 지 결정하게 되는 형태로 구현된다.
![image](https://user-images.githubusercontent.com/38639633/111031883-171c2200-844d-11eb-831c-4cbc7649baa3.png){:.center}

- Faster R-CNN과 비교하였을 때, 단지 Mask FCN Predictor와 RoIAlign만을 추가함으로써 성능 개선을 이끌어냈고, instance segmentation이 가능하게 되었다. 
- 이러한 확장 가능성을 보고, mask branch 뿐만 아니라 keypoint branch를 만들어서 사람의 pose를 추정하는 것도 가능하다는 것도 보여주었다. 

<br>

### YOLACT(You Only Look At CoefficienTs)

**two-stage 구조**의 Mask R-CNN이 있었다면 `YOLOACT`는 real-time이 가능한 **single-stage 구조**의 대표 instane segmentation 모델이다. 

> single-stage와 two-stage의 차이는 바로 이전 포스트에 나와있지만, 다시 설명하자면 bounding box를 추출하고 segmentation을 진행하느냐, 혹은 한번에 미분 가능하도록 네트워크를 설계 후 추출과 segmentation을 한번에 진행하냐의 차이이다.

![image](https://user-images.githubusercontent.com/38639633/111032745-12f20380-8451-11eb-913d-9dbb3b57fb10.png){:.center}

- 기본 backbone 구조는 featrue pyramid를 사용한다. 이를 통해 고해상도의 feature map을 사용할 수 있게된다. 

![image](https://user-images.githubusercontent.com/38639633/111032749-1be2d500-8451-11eb-8894-7df77d91ed66.png){:.center}

- 또 하나의 특징은 마스크의 prototype을 가져와서 사용한다. 
- mask R-CNN은 실제로 사용하지 않아도 80개의 클래스를 고려하고 있다고 가정하면, bounding box마다 80개의 독립적 마스크를 한번에 생성해냈다. 이후, classification 된 결과를 참조하여 하나를 참조하는 형태였다. 
- YOLACT의 Prototype은 mask는 아니지만 mask를 합성해 낼 수 있는 기본적인 soft segmentation의 component들을 생성해낸다. 
	- 선형대수에서의 basis라고 생각하면 이해하기 쉽다. Mask를 Span 해낼 수 있는 basis라고 생각하면 된다. 
	- 마스크는 아니지만, 마스크를 추후에 여러개 생성해 낼 수 있는 '재료'를 제공한다고 생각하자
	- Fast R-CNN이 마스크 후보군을 mask branch의 output 갯수만큼 생성해내는 것과의 차이를 구분하자

![image](https://user-images.githubusercontent.com/38639633/111032757-2309e300-8451-11eb-8f5c-21d7fc56adcb.png){:.center}

- 이후 `prediction head`에서는 `protonet`의 output인 prototype들을 잘 합성할 수 있게끔 해주는 계수들을 출력해준다. 
- 이러한 계수들과 prototype들을 선형결합 해주고, 각 detection에 적합한 mask response map을 아래와 같이 생성(Assembly)해준다. 

![image](https://user-images.githubusercontent.com/38639633/111033008-729cde80-8452-11eb-9895-424499ab84c2.png){:.center}

- detection 1은 사람을 보는 경향이 있고, 2는 라켓에 더 집중하는 경향이 있다. 
- 이를 crop하고 적당한 threshold를 지정하여 instance segmentation을 완료한다. 

<br>

### YolactEdge

YOLACT가 real time이 가능했지만, 조금 더 소형화된 edge 모델로 구현하기 위해 개선된 모델이 `YolactEdge`이다. 간단히 설명하고 넘어가자면, 아래와 같이 keyframe의 feature를 다음 frame에 전달해서 특징맵의 계산량을 획기적으로 줄인 아키텍쳐이다. 

![image](https://user-images.githubusercontent.com/38639633/111033144-1f775b80-8453-11eb-9a99-586f1f01c76b.png){:.center}

- 소형화된 모바일 등에서도 빠른 속도로 동작하면서 성능은 기존 방법과 유사하게 확보할 수 있는 모델들이 개발되었다. 

<br>

# Panoptic segmentation

## What is panoptic segmentation?

Instance segmentation은 배경에는 관심이 없고, 움직이는 작업 물체에 대해서만 segmentation을 진행했다. 하지만, panoptic segmentation의 경우, 이미지 내 **모든** 부분에 대하여 segmentation을 진행하는 구조이다. 

![image](https://user-images.githubusercontent.com/38639633/111037145-37a4a600-8466-11eb-9610-0e52c099e868.png){:.center}

- 배경 정보 뿐만 아니라 instance까지 구분하는 architecture 혹은 task를 의미한다. 

<br>

## UPSNet & VPSNet

### UPSNet

 ![image](https://user-images.githubusercontent.com/38639633/111037752-dfbb6e80-8468-11eb-9a9c-14013152d2b9.png){:.center}

- 구조는 우선 FPN 구조를 사용하여 고해상도의 feature를 얻는다 
- head branch를 여러개로 나눈다.
	- `semantic head` : fully convolution 구조로 되어있고, semantic map을 prediction하게된다.
	- `Instance head` : mask R-CNN과 비슷하게 Class, box, mask branch를 통해 mask logit을 구성한다.
- 이후 `Panoptic Head`를 통해 하나의 segmentation map으로 합쳐준다. 



instance, semantic head 부터 마지막 panoptic head 부분까지의 과정을 조금 더 자세히 살펴보자

![image](https://user-images.githubusercontent.com/38639633/111038249-52c5e480-846b-11eb-816c-a8db68219fac.png){:.center}

- Instance head로부터 나온 $Y_i$는 그야말로 instance의 mask이다. 
- Semantic head로부터 나온 $X_{\text{thing}}$은 물체의 mask, $X_{\text{stuff}}$은 배경을 예측하는 mask이다. 
	- 참고로 UPSNet에서 사용되는 `thing`과 `stuff`는 그 정의가 먼저 필요한데, 논문에서는 thing은 instance segmentation이 구분할 수 있는 셀수 있는(countable objects) 물체를 지칭한다. stuff는 semantic segmentation을 통해 다룰 수 있는 무정형이며 셀수 없는 영역(amorphous and uncountable regions)을 지칭한다. 
- 다음으로 $X_{\text{stuff}}$는 최종 출력의 $N_{\text{stuff}}$으로 바로 들어간다.
- $Y_i$를 bounding box가 아닌 전체 영상의 해당 위치에 넣기 위해서 보강하기 위해서 semantic head의 $X_{\text{thing}}$ 부분, 즉 물체부분을 mask하여 이 response를 $Y_i$와 더해준 뒤 최종 출력에 삽입한다. 
- instance와 배경 이외에도 어디에도 소속되지 않은 unknown class의 물체들도 존재한다. 이를 고려하기 위해서 물체의 semantic mask map에 instance로 사용된 부분들을 제외하여 나머지 배타적인 부분을 모두 unknown으로 추가하여 출력한다. 

<br>

### VPSNet (for video)

real time이 가능하게 만든 VPSNet의 architecture를 간단히 살펴보면 다음의 흐름으로 진행된다. 

1. Align reference features onto the target featrue map (Fusion at pixel level)
2. Track module associates different object instances(Track at instance level)
3. Fused-and-tracked modules are trained to synergize each other

![image](https://user-images.githubusercontent.com/38639633/111059942-a45a8780-84dc-11eb-9baf-f0a6ce02aa6e.png){:.center}

- 두 시간 차이를 가지는 영상 사이에 $\phi^{\text{init}}$라는 motion map을 사용해서, 각 프레임으로부터 나온 feature map을 motion에 따라서 warpping을 해주게 된다. 바꿔 말하면, $t-\tau$시간에서의 모든 픽셀에 대한 $t$ 시간에서의 대응점을 추척하고 그 위치를 찾는 방식이다. 
- $t-\tau$에서 뽑힌 feature이지만, 현재 target frame인 $t$에서 찍은 것과 마찬가지로 tracking을 해준다.  

![image](https://user-images.githubusercontent.com/38639633/111059965-d23fcc00-84dc-11eb-992e-049dd74a93ef.png){:.center}

- 기존 RoI들($m$ gt-roi feats)과 현재 RoI들($n$ roi feats)의 연관성을 찾아 $m\times n$ obj match learning을 가능하게 해주는 matrix를 새엇ㅇ해준다. (`Track head`)
- 이 때 포인트는 각 frame에서 object들의 id를 추적해준다는 점이다. ID를 추적해줌으로써, instance segmentation의 역할 뿐만 아니라, 해당 segmentation이 '같은 종류의 id'로 matching되게 도와준다. 

![image](https://user-images.githubusercontent.com/38639633/111060025-4b3f2380-84dd-11eb-97e0-f1b5e5e3da51.png){:.center}

- 나머지 부분은 UPSNet과 동일하게 진행된다. 각 head에서 나오는 결과들을 panoptic map으로 출력해준다. 



![panoptic](../../assets/img/boostcamp/panoptic.gif)

- (좌측) : Image panoptic segmentation, (우측) : Video panoptic segmentation(VPSNet)
- 이미지 수준의 panoptic에서는 같은 object임에도 프레임에 따라 class id가 계속적으로 바뀌는 모습을 볼 수 있다. 
- 그에 반해 VPSNet의 경우 id를 유지하는 모습을 보인다. 

<br>	

# Landmark localization

segmentation처럼 각 픽셀별 classification을 진행하지만 또 다른 중요한 task 중 하나인 landmark localization을 알아보자. 

<br>

## What is landmark localization

주로 Facial landmark localization 혹은 human pose estimation 분야에 많이 사용된다. 얼굴 혹은 사람의 특정 물체에 대하여 중요하다고 생각되는 특징 부분들(아래 그림에서 point 부분 `=landmark`)을 추정하고 추적하는 task를 의미한다. 

![image](https://user-images.githubusercontent.com/38639633/111062244-0d48fc00-84eb-11eb-9edb-66de5d1b0ea2.png){:.center}

- `Landmark localization(=keypoint estimation)` :  predicting the coordinates of keypoints

<br>

## Coordinate regression vs. heatmap classification

![image](https://user-images.githubusercontent.com/38639633/111069959-d555af00-8512-11eb-974e-4e3a26321041.png){:.center}

- `Coordinate regression` : usually inaccurate and biased
	- box regression처럼 각 포인트의 x,y 좌표를 2N개에 대하여 regression을 진행하는 방식이다. 
	- 하지만, 부정확하고 일반화에 어려움이 있다는 단점이 있다. 

- `Heatmap classification` : better performance but high computational cost
	- 각 채널이 하나의 keypoint를 담당하게 되고, keypoint마다 하나의 클래스로 생각해서 키포인트가 발생할 확률맵을 각 픽셀별로 classification하는 방법으로 해결하는 방식이 제시되었다.  

		![image](https://user-images.githubusercontent.com/38639633/111070103-70e71f80-8513-11eb-9d59-b3ce4b66e5d3.png){:.center}

	- 이 방법은 성능이 좋지만, 모든 픽셀에 대하여 계산하여야 하기 때문에 계산량이 많다는 단점이 있다. 



**Landmark location to Gaussian heatmap**

여기서 heatmap의 표현은 각 위치마다(픽셀마다)의 confidence가 나오는 형태의 표현이다. 이 때, $(x, y)$의 location pair를 어떻게 heatmap으로 표현할지에 대한 내용을 알아보자
우선 식을 살펴보면 다음과 같다. 

$$
G_\sigma(x,y) = \text{exp}\left(-\frac{(x-x_c)^2 + (y-y_c)^2}{2\sigma^2}\right)
$$

- 여기서 $(x_c, y_c)$는  center location을 의미한다.

이를 그래프로 시각화하면 다음과 같다. 

![image](https://user-images.githubusercontent.com/38639633/111070363-c7089280-8514-11eb-8cbc-89419f8de92d.png){:.center}

- 위와 같이 center location을 평균으로 하는 Gaussian 분포를 씌운다. 
- 이를 코드로 구현하면 다음과 같다. 
	![image](https://user-images.githubusercontent.com/38639633/111070455-2d8db080-8515-11eb-957c-80594dc81ebf.png){:.center}

	- 위 코드에서는 임의의 크기로 size를 정사각형 사이즈로 가정하였다
	- 이 사이즈 만큼 가로, 세로 사이즈로 x와 y를 배열로 정의한다. 
	- 임의의 초기값 x0과 y0를 정의하고 가우시안을 씌워준다. 
	- python에서는 가로 배열 x와 세로 배열 y를 더하면 브로드캐스팅에 의해 len(x) by len(y)의 사이즈인 matrix로 출력된다.
	- if문의 계산 속에서 이 같은 규칙이 사용된다. 즉, exponention의 분자의 계산 값 형태는 2차원 배열이다. 

<br>

## Hourglass network

![image](https://user-images.githubusercontent.com/38639633/111073017-6bdc9d00-8520-11eb-95a8-6ac70f3d427f.png){:.center}

- 2016년도에 처음 등장한 이 논문은 landmark localizatino task를 위한 첫번째 논문이다. 
- U-Net과 비슷한 형식의 구조가 여러번 쌓인 이 구조는 모래시계를 닮았다해서 hourglass라 부르고, 전체 모델을 `Stacked hourglass module`이라고도 부른다.
- Stacked hourglass module allows for repeated bottom-up and top-down inference that refines the output of the previous hourglass module
- 이렇게 구조를 만든 이유는 다음과 같은 이유에서이다. 
	- 영상을 전반적으로 작게 만듦으로써 receptive field를 키우고, 이를 통해 큰그림을 보기 위함이다. 
	- 이것을 기반으로 landmark를 찾는 방식이다. 
	- receptive field를 크게 가져가서 큰 영역을 보면서도, skip connection이 존재하여 low level feature를 참조하여 정확한 위치를 특정하게끔 유도하였다.  
- 이 방식을 여러번 거치면서 큰그림과 디테일을 구체화하며 결과를 개선해 나아가는 방식으로 이루어져있다. 
- 현재 주어진 rough한 구조를 정교하게 더정교하게 ... 학습한다.



여기서 하나의 hourglass stack 구조 하나는 U-Net과 약간은 차이점이 있다. 이를 살펴보면 다음과 같다. 

![image](https://user-images.githubusercontent.com/38639633/111073318-9c710680-8521-11eb-98bf-1fead0c0ce0c.png){:.center}

- unet과의 가장 대표적인 차이점은 skip connection의 합쳐지는 방식의 차이이다.
- unet이 concatenate였다면, hourglass는 단순 `+ 연산`이다.
	- 따라서 dimension이 unet처럼 점차적으로 늘어나지 않는다. 
	- 대신, skip할 때 하나의 convolution layer를 통과해서 전달되게 된다. 
- hourglass는 오히려 unet보다 FPN에 조금 더 가깝다고 볼 수 있다. (+연산 때문에!)

<br>

## Extensions

### DensePose

![image](https://user-images.githubusercontent.com/38639633/111073562-9d566800-8522-11eb-935f-b6bb5c9ee532.png){:.center}

- dense하게 온몸의 랜드마크를 추적하는 모델
- 이말은 즉, 3D modeling이 가능함을 의미한다. 

![image](https://user-images.githubusercontent.com/38639633/111073612-c0811780-8522-11eb-826f-e902c7bbbe16.png){:.center}

- UV map is a flattened representation of 3D geometry Also, UV map is invariant to motion( i.e., canonical coordinate)

![image](https://user-images.githubusercontent.com/38639633/111073645-d7276e80-8522-11eb-8dfb-e3174a9ab0f4.png){:.center}

- DensePose R-CNN = Faster R-CNN + 3D surface regression branch

<br>

### RetinaFace

![image](https://user-images.githubusercontent.com/38639633/111073693-f7efc400-8522-11eb-826b-aeade767c206.png){:.center}

- ReinaFace = FPN + Multi-task branches(classification, bounding box, 5 point regression, mesh regression)
<br>

### Extension pattern?

![image](https://user-images.githubusercontent.com/38639633/111073727-1786ec80-8523-11eb-8bc9-35552ee78a4f.png){:.center}

- FPN + Target-task branches

<br>


# Detecting objects as keypoints

앞서 object detection을 공부했던 이전 포스팅에서 bounding box가 아닌, keypoint를 통해 detecting을 하는 방법들을 살짝 언급했었는데, 이러한 방식의 모델에 대하여 설명하도록 한다. 

<br>

## CornerNet & CenterNet

### CornerNet

**Bounding box**=\{Top-left, bottom-right\} corners 로 바운딩 박스를 구성하고, 이 keypoint를 추적한다.

![image](https://user-images.githubusercontent.com/38639633/111074010-887ad400-8524-11eb-944d-39a2ab8379ab.png){:.center}

- 위 그림과 같이 backbone 네트워크를 통과한 feature map을 4가지 head를 통해 bounding box를 검출하는 과정을 거친다
- head의 종류는 다음과 같다. 
	- `top left corner의 heatmap`, `bottom right corner의 heatmap`을 통해 각각의 object의 unique한 bounding box들을 특정한다.
	- `Embedding`이라는 각 포인트의 정보를 포함하는 head를 하나 더 설정한다.
	- 학습할 때 두 코너에서 나오는 embedding point는 같은 object에서 나왔다는 점을 활용하여 제한 조건을 설정한다. 
- 요약하면, Heatmap head를 통해 모든 corner point만 잔뜩 출력해주고, 다음으로 embedding matching을 통해서 bounding box로 추출이 가능하도록 해주는 구조이다. (corner들의 pair를 matching)

<br>


### CenterNet (1)

**Bounding box**=\{Top-left, bottom-right, Center\} points

cornernet에 cener point를 추가하여 바운딩 박스를 선정하는 방식으로 이뤄져있다. 

![image](https://user-images.githubusercontent.com/38639633/111089476-69556400-856f-11eb-8f37-2ac3cc34d294.png){:.center}


<br>



### CenterNet(2)

**Bounding box**=\{Width, Height, Center\} points

![image](https://user-images.githubusercontent.com/38639633/111089554-c51fed00-856f-11eb-8e81-d8ed3ffeee1f.png){:.center}



위 두 centerNet의 경우 큰 차이는 없으며, center point를 기준으로 corner point 혹은 너비와 높이로 bounding box를 특정하는데 그 의의를 가진다. 

centerNet(2)를 기준으로 그 성능은 기존 object detection들과 비교했을 때 더 좋은 성능을 보이는 것을 확인할 수 있다. 

![image](https://user-images.githubusercontent.com/38639633/111089611-ee407d80-856f-11eb-80e8-87bd1663de91.png){:.center}

<br>


# Reference

**Instance segmentation**

- Kirillov et al., Panoptic segmentation CVPR 2019
- He et al., Mask R-CNN ICCV 2017
- Bolya et al., YOLACT Real time Instance Segmentation, ICCV 2019
- Liu et al., YolactEdge: Real time Instance Segmentation on the Edge arXiv 2020

**Panoptic segmentation**

- Xiong et al., UPSNet: A Unified Panoptic Segmentation Network, CVPR 2019
- Kim-et-al.,-Video-Panoptic-Segmentation,-CVPR-2020

**Landmark localization**

- Cao et al., OpenPose: Realtime Multi Person 2D Pose Estimation using Part Affinity Fields, IEEE TPAMI 2019
- Jin et al., Pixel in Pixel Net: Towards Efficient Facial Landmark Detection in the Wild, arXiv 2020
- Wang et al., Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression, ICCV 2019
- Newell et al., Stacked Hourglass Networks for Human Pose Estimation, ECCV 2016
- Guler et al., DensePose: Dense Human Pose Estimation in the Wild, CVPR 2018

**Detecting objects as keypoints**

- Law et al., CornerNet: Detecting Objects as Paired Keypoints ECCV 2018
- Duan et al., CenterNet: Keypoint Triplets for Object Detection, ICCV 2019
- Zhou et al., Objects as Points, arXiv 2019



**Further Question**

(1) Mask R-CNN과 Faster R-CNN은 어떤 차이점이 있을까요? (ex. 풀고자 하는 task, 네트워크 구성 등)
(2) Panoptic segmentation과 instance segmentation은 어떤 차이점이 있을까요?
(3) Landmark localization은 human pose estimation 이외의 어떤 도메인에 적용될 수 있을까요?

 

