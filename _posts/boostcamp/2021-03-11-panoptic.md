---
layout: post
title: CV / Instance, Panoptic segmentation
subtitle: Adios object detection...
thumbnail-img: https://user-images.githubusercontent.com/38639633/110747931-6530fa00-8282-11eb-838c-c4952c98f019.png
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [BOOSTCAMP]
tags: [boostcamp]
toc: true
comments: true
---



단순히 픽셀 마다의 클래스를 분류하는 semantic segmentation은 **동일한 클래스에 속하는 개별 물체**를 구분하지 못합니다. 이와 달리 instance segmentation은 영상 내에 동일한 물체가 여러 개 존재하는 경우에 각각의 물체를 구분하며 동시에 픽셀 단위의 mask도 예측하는 방법입니다. 그리고 semantic segmentation과 instance segmentation을 결합하여 더욱 복잡한 task인 panoptic segmentation을 소개합니다.

또 다른 물체를 인식하는 방법에는 각 물체를 대표하는 점들을 예측하는 것이 있습니다. 이러한 task를 landmark localization이라고 하며 사람의 동작을 인식하는 human pose estimation에 주로 사용되고 있습니다. Landmark localization을 대표하는 모델인 hourglass를 위주로 해당 task를 소개합니다.

 

# Instance segmentation

## What is instance segmentation?

기존 픽셀을 같은 클래스로만 분류하는 Semantic segmentation에서 더욱 발전한 모델인 instance segmentation을 알아보자. 

`Instance segmentation`은 말 그대로 개체간의 분류도 이뤄지는 segmentation이라고 할 수 있다. 

![image-20210311153416903](../../assets/img/boostcamp/image-20210311153416903.png)

- (좌측) 이미지를 segmentation 할 때, 
- (가운데) 이미지는 원본 이미지를 단순 semantic segmentation한 결과이다. 
- (우측) 이미지는 여기서 한발짝 더 나아가, 의자간의 분류, 그러니깐 instance까지도 분류하는 모습이다. 



## Instance segmenters

그렇다면 instance segmentation을 가능케하는 다양한 모델에 대하여 살펴보자.

### Mask R-CNN

![image](https://user-images.githubusercontent.com/38639633/111031850-f2c04580-844c-11eb-9978-34c61bcc7095.png)

- 기존 좋은 성능을 냈던, Faster R-CNN에서 `RoIAlign`이라는 새로운 pooling 방식을 추가하였다. 

	![image](https://user-images.githubusercontent.com/38639633/111031742-5b5af280-844c-11eb-9758-54baef1c37f6.png)

- Faster R-CNN은 RoI Pooling이라는 방법을 사용하여 정수 좌표에서만 feature를 뽑아왔었다. 즉, 정수가 아닌 부분에서 feature는 근사하여 뽑혔다. 

- RoIAlign은 interpolation을 통해 소수점에서도 pooling을 지원하게 된다. 이를 통해 성능improvement로 이어지게된다. 

- 또 기존의 Faster R-CNN이 마지막에 Class와 box regression branch를 사용했던 것과는 달리, 추가적으로 `mask branch`를 도입하였다. 

	![image](https://user-images.githubusercontent.com/38639633/111031752-6f065900-844c-11eb-9d3a-ac38233b7df7.png)

- 이 mask brach를 통해 binary 마스크를 classification하도록 하는 구조를 갖는다. 

- 위 그림에서는 총 80개의 클래스를 고려하도록 한다. (80개에 대하여 기인지 아닌지)

- 일괄적으로 모든 마스크를 생성하고, 다른 branch인 class head에서 나온 결과를 이용해서 어떤 마스크를 사용할 지 결정하게 되는 형태로 구현된다.

![image](https://user-images.githubusercontent.com/38639633/111031883-171c2200-844d-11eb-831c-4cbc7649baa3.png)

- Faster R-CNN과 비교하였을 때, 단지 Mask FCN Predictor와 RoIAlign만을 추가함으로써 성능 개선을 이끌어냈고, instance segmentation이 가능하게 되었다. 
- 이러한 확장 가능성을 보고, mask branch 뿐만 아니라 keypoint branch를 만들어서 사람의 pose를 추정하는 것도 가능하다는 것도 보여주었다. 



### YOLACT(You Only Look At CoefficienTs)

**two-stage 구조**의 Mask R-CNN이 있었다면 `YOLOACT`는 real-time이 가능한 **single-stage 구조**의 대표 instane segmentation 모델이다. 

> single-stage와 two-stage의 차이는 바로 이전 포스트에 나와있지만, 다시 설명하자면 bounding box를 추출하고 segmentation을 진행하느냐, 혹은 한번에 미분 가능하도록 네트워크를 설계 후 추출과 segmentation을 한번에 진행하냐의 차이이다.

![image](https://user-images.githubusercontent.com/38639633/111032745-12f20380-8451-11eb-913d-9dbb3b57fb10.png)

- 기본 backbone 구조는 featrue pyramid를 사용한다. 이를 통해 고해상도의 feature map을 사용할 수 있게된다. 

![image](https://user-images.githubusercontent.com/38639633/111032749-1be2d500-8451-11eb-8894-7df77d91ed66.png)

- 또 하나의 특징은 마스크의 prototype을 가져와서 사용한다. 
- mask R-CNN은 실제로 사용하지 않아도 80개의 클래스를 고려하고 있다고 가정하면, bounding box마다 80개의 독립적 마스크를 한번에 생성해냈다. 이후, classification 된 결과를 참조하여 하나를 참조하는 형태였다. 
- YOLACT의 Prototype은 mask는 아니지만 mask를 합성해 낼 수 있는 기본적인 soft segmentation의 component들을 생성해낸다. 
	- 선형대수에서의 basis라고 생각하면 이해하기 쉽다. Mask를 Span 해낼 수 있는 basis라고 생각하면 된다. 
	- 마스크는 아니지만, 마스크를 추후에 여러개 생성해 낼 수 있는 '재료'를 제공한다고 생각하자
	- Fast R-CNN이 마스크 후보군을 mask branch의 output 갯수만큼 생성해내는 것과의 차이를 구분하자

![image](https://user-images.githubusercontent.com/38639633/111032757-2309e300-8451-11eb-8f5c-21d7fc56adcb.png)

- 이후 `prediction head`에서는 `protonet`의 output인 prototype들을 잘 합성할 수 있게끔 해주는 계수들을 출력해준다. 
- 이러한 계수들과 prototype들을 선형결합 해주고, 각 detection에 적합한 mask response map을 아래와 같이 생성(Assembly)해준다. 

![image](https://user-images.githubusercontent.com/38639633/111033008-729cde80-8452-11eb-9895-424499ab84c2.png)

- detection 1은 사람을 보는 경향이 있고, 2는 라켓에 더 집중하는 경향이 있다. 
- 이를 crop하고 적당한 threshold를 지정하여 instance segmentation을 완료한다. 



### YolactEdge

YOLACT가 real time이 가능했지만, 조금 더 소형화된 edge 모델로 구현하기 위해 개선된 모델이 `YolactEdge`이다. 간단히 설명하고 넘어가자면, 아래와 같이 keyframe의 feature를 다음 frame에 전달해서 특징맵의 계산량을 획기적으로 줄인 아키텍쳐이다. 

![image](https://user-images.githubusercontent.com/38639633/111033144-1f775b80-8453-11eb-9a99-586f1f01c76b.png)

- 소형화된 모바일 등에서도 빠른 속도로 동작하면서 성능은 기존 방법과 유사하게 확보할 수 있는 모델들이 개발되었다. 



# Panoptic segmentation

## What is panoptic segmentation?

Instance segmentation은 배경에는 관심이 없고, 움직이는 작업 물체에 대해서만 segmentation을 진행했다. 하지만, panoptic segmentation의 경우, 이미지 내 **모든** 부분에 대하여 segmentation을 진행하는 구조이다. 

![image](https://user-images.githubusercontent.com/38639633/111037145-37a4a600-8466-11eb-9610-0e52c099e868.png)

- 배경 정보 뿐만 아니라 instance까지 구분하는 architecture 혹은 task를 의미한다. 



## UPSNet & VPSNet

### UPSNet

 ![image](https://user-images.githubusercontent.com/38639633/111037752-dfbb6e80-8468-11eb-9a9c-14013152d2b9.png)

- 구조는 우선 FPN 구조를 사용하여 고해상도의 feature를 얻는다 
- head branch를 여러개로 나눈다.
	- `semantic head` : fully convolution 구조로 되어있고, semantic map을 prediction하게된다.
	- `Instance head` : mask R-CNN과 비슷하게 Class, box, mask branch를 통해 mask logit을 구성한다.
- 이후 `Panoptic Head`를 통해 하나의 segmentation map으로 합쳐준다. 



instance, semantic head 부터 마지막 panoptic head 부분까지의 과정을 조금 더 자세히 살펴보자

![image](https://user-images.githubusercontent.com/38639633/111038249-52c5e480-846b-11eb-816c-a8db68219fac.png)









**Further Question**

(1) Mask R-CNN과 Faster R-CNN은 어떤 차이점이 있을까요? (ex. 풀고자 하는 task, 네트워크 구성 등)
(2) Panoptic segmentation과 instance segmentation은 어떤 차이점이 있을까요?
(3) Landmark localization은 human pose estimation 이외의 어떤 도메인에 적용될 수 있을까요?

 

