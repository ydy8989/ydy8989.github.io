---
layout: post
title: CV / Instance, Panoptic segmentation
subtitle: Adios object detection...
thumbnail-img: https://user-images.githubusercontent.com/38639633/110747931-6530fa00-8282-11eb-838c-c4952c98f019.png
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [BOOSTCAMP]
tags: [boostcamp]
toc: true
comments: true
---



단순히 픽셀 마다의 클래스를 분류하는 semantic segmentation은 **동일한 클래스에 속하는 개별 물체**를 구분하지 못합니다. 이와 달리 instance segmentation은 영상 내에 동일한 물체가 여러 개 존재하는 경우에 각각의 물체를 구분하며 동시에 픽셀 단위의 mask도 예측하는 방법입니다. 그리고 semantic segmentation과 instance segmentation을 결합하여 더욱 복잡한 task인 panoptic segmentation을 소개합니다.

또 다른 물체를 인식하는 방법에는 각 물체를 대표하는 점들을 예측하는 것이 있습니다. 이러한 task를 landmark localization이라고 하며 사람의 동작을 인식하는 human pose estimation에 주로 사용되고 있습니다. Landmark localization을 대표하는 모델인 hourglass를 위주로 해당 task를 소개합니다.

 

# Instance segmentation

## What is instance segmentation?

기존 픽셀을 같은 클래스로만 분류하는 Semantic segmentation에서 더욱 발전한 모델인 instance segmentation을 알아보자. 

`Instance segmentation`은 말 그대로 개체간의 분류도 이뤄지는 segmentation이라고 할 수 있다. 

![image-20210311153416903](../../assets/img/boostcamp/image-20210311153416903.png)

- (좌측) 이미지를 segmentation 할 때, 
- (가운데) 이미지는 원본 이미지를 단순 semantic segmentation한 결과이다. 
- (우측) 이미지는 여기서 한발짝 더 나아가, 의자간의 분류, 그러니깐 instance까지도 분류하는 모습이다. 



## Instance segmenters

그렇다면 instance segmentation을 가능케하는 다양한 모델에 대하여 살펴보자.

### Mask R-CNN

![image](https://user-images.githubusercontent.com/38639633/111031850-f2c04580-844c-11eb-9978-34c61bcc7095.png)

- 기존 좋은 성능을 냈던, Faster R-CNN에서 `RoIAlign`이라는 새로운 pooling 방식을 추가하였다. 

	![image](https://user-images.githubusercontent.com/38639633/111031742-5b5af280-844c-11eb-9758-54baef1c37f6.png)

- Faster R-CNN은 RoI Pooling이라는 방법을 사용하여 정수 좌표에서만 feature를 뽑아왔었다. 즉, 정수가 아닌 부분에서 feature는 근사하여 뽑혔다. 

- RoIAlign은 interpolation을 통해 소수점에서도 pooling을 지원하게 된다. 이를 통해 성능improvement로 이어지게된다. 

- 또 기존의 Faster R-CNN이 마지막에 Class와 box regression branch를 사용했던 것과는 달리, 추가적으로 `mask branch`를 도입하였다. 

	![image](https://user-images.githubusercontent.com/38639633/111031752-6f065900-844c-11eb-9d3a-ac38233b7df7.png)

- 이 mask brach를 통해 binary 마스크를 classification하도록 하는 구조를 갖는다. 

- 위 그림에서는 총 80개의 클래스를 고려하도록 한다. (80개에 대하여 기인지 아닌지)

- 일괄적으로 모든 마스크를 생성하고, 다른 branch인 class head에서 나온 결과를 이용해서 어떤 마스크를 사용할 지 결정하게 되는 형태로 구현된다.

![image](https://user-images.githubusercontent.com/38639633/111031883-171c2200-844d-11eb-831c-4cbc7649baa3.png)

- Faster R-CNN과 비교하였을 때, 단지 Mask FCN Predictor와 RoIAlign만을 추가함으로써 성능 개선을 이끌어냈고, instance segmentation이 가능하게 되었다. 
- 이러한 확장 가능성을 보고, mask branch 뿐만 아니라 keypoint branch를 만들어서 사람의 pose를 추정하는 것도 가능하다는 것도 보여주었다. 



### YOLACT(You Only Look At CoefficienTs)

**two-stage 구조**의 Mask R-CNN이 있었다면 `YOLOACT`는 real-time이 가능한 **single-stage 구조**의 대표 instane segmentation 모델이다. 

> single-stage와 two-stage의 차이는 바로 이전 포스트에 나와있지만, 다시 설명하자면 bounding box를 추출하고 segmentation을 진행하느냐, 혹은 한번에 미분 가능하도록 네트워크를 설계 후 추출과 segmentation을 한번에 진행하냐의 차이이다.

![image](https://user-images.githubusercontent.com/38639633/111032745-12f20380-8451-11eb-913d-9dbb3b57fb10.png)

- 기본 backbone 구조는 featrue pyramid를 사용한다. 이를 통해 고해상도의 feature map을 사용할 수 있게된다. 

![image](https://user-images.githubusercontent.com/38639633/111032749-1be2d500-8451-11eb-8894-7df77d91ed66.png)

- 또 하나의 특징은 마스크의 prototype을 가져와서 사용한다. 
- mask R-CNN은 실제로 사용하지 않아도 80개의 클래스를 고려하고 있다고 가정하면, bounding box마다 80개의 독립적 마스크를 한번에 생성해냈다. 이후, classification 된 결과를 참조하여 하나를 참조하는 형태였다. 
- YOLACT의 Prototype은 mask는 아니지만 mask를 합성해 낼 수 있는 기본적인 soft segmentation의 component들을 생성해낸다. 
	- 선형대수에서의 basis라고 생각하면 이해하기 쉽다. Mask를 Span 해낼 수 있는 basis라고 생각하면 된다. 
	- 마스크는 아니지만, 마스크를 추후에 여러개 생성해 낼 수 있는 '재료'를 제공한다고 생각하자
	- Fast R-CNN이 마스크 후보군을 mask branch의 output 갯수만큼 생성해내는 것과의 차이를 구분하자

![image](https://user-images.githubusercontent.com/38639633/111032757-2309e300-8451-11eb-8f5c-21d7fc56adcb.png)

- 이후 `prediction head`에서는 `protonet`의 output인 prototype들을 잘 합성할 수 있게끔 해주는 계수들을 출력해준다. 
- 이러한 계수들과 prototype들을 선형결합 해주고, 각 detection에 적합한 mask response map을 아래와 같이 생성(Assembly)해준다. 

![image](https://user-images.githubusercontent.com/38639633/111033008-729cde80-8452-11eb-9895-424499ab84c2.png)

- detection 1은 사람을 보는 경향이 있고, 2는 라켓에 더 집중하는 경향이 있다. 
- 이를 crop하고 적당한 threshold를 지정하여 instance segmentation을 완료한다. 



### YolactEdge

YOLACT가 real time이 가능했지만, 조금 더 소형화된 edge 모델로 구현하기 위해 개선된 모델이 `YolactEdge`이다. 간단히 설명하고 넘어가자면, 아래와 같이 keyframe의 feature를 다음 frame에 전달해서 특징맵의 계산량을 획기적으로 줄인 아키텍쳐이다. 

![image](https://user-images.githubusercontent.com/38639633/111033144-1f775b80-8453-11eb-9a99-586f1f01c76b.png)

- 소형화된 모바일 등에서도 빠른 속도로 동작하면서 성능은 기존 방법과 유사하게 확보할 수 있는 모델들이 개발되었다. 



# Panoptic segmentation

## What is panoptic segmentation?

Instance segmentation은 배경에는 관심이 없고, 움직이는 작업 물체에 대해서만 segmentation을 진행했다. 하지만, panoptic segmentation의 경우, 이미지 내 **모든** 부분에 대하여 segmentation을 진행하는 구조이다. 

![image](https://user-images.githubusercontent.com/38639633/111037145-37a4a600-8466-11eb-9610-0e52c099e868.png)

- 배경 정보 뿐만 아니라 instance까지 구분하는 architecture 혹은 task를 의미한다. 



## UPSNet & VPSNet

### UPSNet

 ![image](https://user-images.githubusercontent.com/38639633/111037752-dfbb6e80-8468-11eb-9a9c-14013152d2b9.png)

- 구조는 우선 FPN 구조를 사용하여 고해상도의 feature를 얻는다 
- head branch를 여러개로 나눈다.
	- `semantic head` : fully convolution 구조로 되어있고, semantic map을 prediction하게된다.
	- `Instance head` : mask R-CNN과 비슷하게 Class, box, mask branch를 통해 mask logit을 구성한다.
- 이후 `Panoptic Head`를 통해 하나의 segmentation map으로 합쳐준다. 



instance, semantic head 부터 마지막 panoptic head 부분까지의 과정을 조금 더 자세히 살펴보자

![image](https://user-images.githubusercontent.com/38639633/111038249-52c5e480-846b-11eb-816c-a8db68219fac.png)

- Instance head로부터 나온 $Y_i$는 그야말로 instance의 mask이다. 
- Semantic head로부터 나온 $X_{\text{thing}}$은 물체의 mask, $X_{\text{stuff}}$은 배경을 예측하는 mask이다. 
	- 참고로 UPSNet에서 사용되는 `thing`과 `stuff`는 그 정의가 먼저 필요한데, 논문에서는 thing은 instance segmentation이 구분할 수 있는 셀수 있는(countable objects) 물체를 지칭한다. stuff는 semantic segmentation을 통해 다룰 수 있는 무정형이며 셀수 없는 영역(amorphous and uncountable regions)을 지칭한다. 
- 다음으로 $X_{\text{stuff}}$는 최종 출력의 $N_{\text{stuff}}$으로 바로 들어간다.
- $Y_i$를 bounding box가 아닌 전체 영상의 해당 위치에 넣기 위해서 보강하기 위해서 semantic head의 $X_{\text{thing}}$ 부분, 즉 물체부분을 mask하여 이 response를 $Y_i$와 더해준 뒤 최종 출력에 삽입한다. 
- instance와 배경 이외에도 어디에도 소속되지 않은 unknown class의 물체들도 존재한다. 이를 고려하기 위해서 물체의 semantic mask map에 instance로 사용된 부분들을 제외하여 나머지 배타적인 부분을 모두 unknown으로 추가하여 출력한다. 



### VPSNet (for video)

real time이 가능하게 만든 VPSNet의 architecture를 간단히 살펴보면 다음의 흐름으로 진행된다. 

1. Align reference features onto the target featrue map (Fusion at pixel level)
2. Track module associates different object instances(Track at instance level)
3. Fused-and-tracked modules are trained to synergize each other

![image](https://user-images.githubusercontent.com/38639633/111059942-a45a8780-84dc-11eb-9baf-f0a6ce02aa6e.png)

- 두 시간 차이를 가지는 영상 사이에 $\phi^{\text{init}}$라는 motion map을 사용해서, 각 프레임으로부터 나온 feature map을 motion에 따라서 warpping을 해주게 된다. 바꿔 말하면, $t-\tau$시간에서의 모든 픽셀에 대한 $t$ 시간에서의 대응점을 추척하고 그 위치를 찾는 방식이다. 
- $t-\tau$에서 뽑힌 feature이지만, 현재 target frame인 $t$에서 찍은 것과 마찬가지로 tracking을 해준다.  

![image](https://user-images.githubusercontent.com/38639633/111059965-d23fcc00-84dc-11eb-992e-049dd74a93ef.png)

- 기존 RoI들($m$ gt-roi feats)과 현재 RoI들($n$ roi feats)의 연관성을 찾아 $m\times n$ obj match learning을 가능하게 해주는 matrix를 새엇ㅇ해준다. (`Track head`)
- 이 때 포인트는 각 frame에서 object들의 id를 추적해준다는 점이다. ID를 추적해줌으로써, instance segmentation의 역할 뿐만 아니라, 해당 segmentation이 '같은 종류의 id'로 matching되게 도와준다. 

![image](https://user-images.githubusercontent.com/38639633/111060025-4b3f2380-84dd-11eb-97e0-f1b5e5e3da51.png)

- 나머지 부분은 UPSNet과 동일하게 진행된다. 각 head에서 나오는 결과들을 panoptic map으로 출력해준다. 



![panoptic](../../assets/img/boostcamp/panoptic.gif)

- (좌측) : Image panoptic segmentation, (우측) : Video panoptic segmentation(VPSNet)

- 이미지 수준의 panoptic에서는 같은 object임에도 프레임에 따라 class id가 계속적으로 바뀌는 모습을 볼 수 있다. 

- 그에 반해 VPSNet의 경우 id를 유지하는 모습을 보인다. 

	

# Landmark localization

segmentation처럼 각 픽셀별 classification을 진행하지만 또 다른 중요한 task 중 하나인 landmark localization을 알아보자. 

## What is landmark localization









**Further Question**

(1) Mask R-CNN과 Faster R-CNN은 어떤 차이점이 있을까요? (ex. 풀고자 하는 task, 네트워크 구성 등)
(2) Panoptic segmentation과 instance segmentation은 어떤 차이점이 있을까요?
(3) Landmark localization은 human pose estimation 이외의 어떤 도메인에 적용될 수 있을까요?

 

