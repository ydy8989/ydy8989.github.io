---
layout: post
title: Node2Vec and Latent Factor Model
subtitle: Netflix Challenge with latent factor model 
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [BOOSTCAMP]
tags: [boostcamp, recommendation]
comments: true
---

그래프의 정점을 벡터로 표현하는 방법인 정점 임베딩(Node Embedding)에 대해서 배웁니다. 기계학습의 다양한 툴은 벡터로 표현된 데이터를 입력으로 받습니다. 이번 강의에서는 **그래프의 정점(Node)을 벡터로 표현하는 방법**인 정점 임베딩(Node Embedding)에 대해 배웁니다. **정점을 어떻게 벡터로 표현하는지, 정점 사이의 유사성을 어떻게 표현하는지 집중**하며 공부합니다.



# Node Representation Learning

>  In this section, we study several methods to represent a graph in the embedding space. By “embedding” we mean mapping each node in a network into a low-dimensional space, which will give us insight into nodes’ similarity and network structure. Given the widespread prevalence of graphs on the web and in the physical world, representation learning on graphs plays a significant role in a wide range of applications, such as link prediction and anomaly detection. However, modern machine learning algorithms are designed for simple sequence or grids (e.g., fixed-size images/grids, or text/sequences), networks often have complex topographical structures and multimodel features. We will explore embedding methods to get around the difficulties.



## What is Node Representation?

정점 표현학습 : 그래프의 Vertices를 vector로 표현하는 것

![image](https://user-images.githubusercontent.com/38639633/109171263-f43f0c00-77c4-11eb-8336-b5b93eb25c19.png)

- 정점 표현 학습(Node Representation Learning)은 `정점 임베딩(Node Embedding)`이라고도 부른다.
- node embedding은 vector 형태의 표현 그 자체를 의미하기도 한다.
- 정점이 표현되는 vector space를 `Embedding Sapce`라고  부른다.
- node representation learning의 `입력(input)`은 graph이다. 
- 주어진 graph의 각 vertex $u$에 대한 embedding, 즉 vector representation $z_u$는 node embedding의 `output`이다.

	![image](https://user-images.githubusercontent.com/38639633/109172819-8693df80-77c6-11eb-8fe6-95cfd569f708.png)



## The reasons of Node embedding

- node embedding을 진행함으로써 vector 형태의 데이터를 위한 도구를 그래프에 적용할 수 있다. 
	- 많은 분류기, Clustering 알고리즘 등은 벡터 형태로 표현된 instance를 입력으로 받는다.
	- 그래프 정점 분류, 군집 분석 등에 사용할 수 있다. 



## Goal of Node embedding

Node embedding의 목표는 임베딩 공간에서의 유사성이 원래 graph network에서의 유사성과 유사하도록 node를 encoding하는 것에 그 목표를 두고 있다. 즉, graph 내에서 유사도가 높은 두 점은 임베딩 공간에서도 유사도가 높도록 하는 것이 목표이다. 

> The goal of node embedding is to encode nodes so that similarity in the embedding space (e.g., dot product) approximates similarity in the original network, the node embedding algorithms we will explore generally consist of three basic stages:

![image](https://user-images.githubusercontent.com/38639633/109244994-0b144b80-7823-11eb-89ac-211a00b8f6e3.png)



앞서 node embedding의 목표를 graph와 embedding space에서 유사도를 보존하기 위함으로 설명했다. 그렇다면 여기서 말하는 Graph structure와 embedding space에서의 `유사도`는 각각 어떻게 정의될까?

- Embedding space : **Inner product($z_v^\top z_u = \vert\vert z_u\vert\vert\cdot\vert\vert z_v\vert\vert\cdot cos(\theta)$)**로 유사도를 표현한다. 
- Graph structure : **similarity($u,v$)**
	- 이 때, Graph에서의 두 정점간 유사도를 정의하는 방식은 여러가지가 있다. 
	- 본 포스팅에서는 `Adjency-base`, `distance-based`, `path-based`, `nesting-based`, `random walk based`에 대해 간략히 소개한다.



결국, 정리하자면 Node embedding은 다음 두 단계로 이루어진다고 할 수 있다. 

1. 그래프 구조에서의 similarity define
2. step 1.에서 정의한 similarity가 $z_v^\top z_u$로 수렴하도록 학습하는 단계. 즉, 이 부분에 해당하는 내용을 Loss function으로 사용한다고 생각하면 이해하기 쉽다. 



### Adjacency 기반 접근법

두 정점이 인접할 때 유사하다고 간주한다. 즉, 두 정점 $u, v$가 인접하다는 것은 두 정점을 연결하는 edge $(u, v)$가 있음을 의미한다. 









 **Further Reading**

- [https://arxiv.org/pdf/1607.00653.pdf](https://arxiv.org/pdf/1607.00653.pdf)
- [https://arxiv.org/pdf/1403.6652.pdf](https://arxiv.org/pdf/1403.6652.pdf)

 