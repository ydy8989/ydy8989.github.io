---
layout: post
title: Self-supervised Pre-training Models
subtitle: GPT-1과 BERT 그리고 이후의 최신 모델들
thumbnail-img : https://user-images.githubusercontent.com/38639633/108630616-04937600-74a9-11eb-95bd-8d0a74e7dff0.png
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [BOOSTCAMP]
tags: [boostcamp,transformer, machine translation]
comments: true
---

자연어 처리 분야에 한 획을 그은 **GPT-1**과 **BERT**를 소개합니다. **GPT-1**과 **BERT**는 **Transfer Learning, Self-supervised Learning, Transformer**를 사용했다는 공통점이 있습니다. 세가지의 강력한 무기를 이용해 대용량의 text를 학습한 모델을 target task에 적용해 거의 모든 기존 자연어처리 task를 압도하는 성능을 보여주었습니다. 세 가지의 키워드를 통해 두 모델을 자세히 알아봅니다.

또한, 이후에는 GPT 시리즈가 2와 3로 이어지면서 일부 데이터셋/task에 대해서는 사람보다 더 뛰어난 작문 능력을 보여주기도 합니다. 이로 인해, model size 만능론이 등장하며 resource가 부족한 많은 연구자들을 슬프게 만들기도 했습니다. 다른 연구 방향으로 transformer의 parameter를 조금 더 효율적으로 활용하고 더 나은 architecture/pre-training task를 찾고자 하는 ALBERT와 ELECTRA에 대해서 알아봅니다. 두 모델 모두 풍부한 실험과 명확한 motivation으로 많은 연구자들의 관심을 받은 논문입니다.

위에서 설명드린 연구방향과는 또 다른 연구 흐름으로 경량화 모델/사전 학습 언어 모델을 보완하기 위한 지식 그래프 integration에 대해 소개한 논문들을 간략하게나마 알아봅니다. 



# Self-supervised Pre-training Models

**Recent Trends**

- Transformer model and its self-attention block has become a general-purpose sequence (or set) encoder and decoder in recent NLP applications as well as in other areas.
- Training deeply stacked Transformer models via a self-supervised learning framework has significantly advanced various NLP tasks through transfer learning, e.g., BERT, GPT-3, XLNet, ALBERT, RoBERTa, Reformer, T5, ELECTRA…
- Other applications are fast adopting the self-attention and Transformer architecture as well as self-supervised learning approach, e.g., recommender systems, drug discovery, computer vision, …
- As for natural language generation, self-attention models still requires a greedy decoding of
	words one at a time.

transformer 이후부터는 self-attention 블럭을 최대한 활용하여 모델을 구성하는 모습을 보이고 있다. 특히, 기존의 6개층으로 쌓은 transformer에서 더 나아가 12개 24개층으로 쌓는 모델들이 개발되고 있으며, self-supervised learning framework의 형태로 발전하는 모습을 보이고 있다. 특히, 이는 신약개발, 추천시스템 등 다양한 도메인에서 활용되고 있다. 

하지만, NLU 분야에서는 아직 greedy한 디코딩 방식에서 벗어나지 못하는 모습을 보이고 있다. 



## GPT-1

일론머스크가 세운 비영리 연구기관 OPENAI에서 발표한 모델이며 최근 2,3로 human score를 넘는 성능을 보여주고 있는 모델이다. 

- It introduces special tokens, such as `<S> /<E>/ $`, to achieve effective transfer learning
	during fine-tuning
- It does not need to use additional task-specific architectures on top of transferred

GPT-1은 다양한 토큰을 이용해 심플한 task뿐만 아니라 다양한 task를 동시에 커버할 수 있는 통합된 모델을 제안했다는 것이 특징이다. 

![image](https://user-images.githubusercontent.com/38639633/108632455-69070300-74b2-11eb-80b2-71ba5edf0891.png)

seq2seq가 별도로 있는 것이 아니라, 영어 데이터로 이루어진 많은 데이터를 다운 받아서 추출된 문장으로부터   















**Further Reading**

- [GPT-1](https://openai.com/blog/language-unsupervised/)
- [BERT : Pre-training of deep bidirectional transformers for language understanding, NAACL’19](https://arxiv.org/abs/1810.04805)
- [SQuAD: Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/)
- [SWAG: A Large-scale Adversarial Dataset for Grounded Commonsense Inference](https://leaderboard.allenai.org/swag/submissions/public)
- [How to Build OpenAI’s GPT-2: “ The AI That Was Too Dangerous to Release”](https://blog.floydhub.com/gpt2/)
- [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, ICLR’20](https://arxiv.org/abs/1909.11942)
- [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, ICLR’20](https://arxiv.org/abs/2003.10555)
- [DistillBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, NeurIPS Workshop'19](https://arxiv.org/abs/1910.01108)
- [TinyBERT: Distilling BERT for Natural Language Understanding, Findings of EMNLP’20](https://arxiv.org/abs/1909.10351)
- [ERNIE: Enhanced Language Representation with Informative Entities, ACL'19](https://arxiv.org/abs/1905.07129)
- [KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning, EMNLP'19](https://arxiv.org/abs/1909.02151)



**Further Question**

- BERT의 Masked Language Model의 단점은 무엇이 있을까요? 사람이 실제로 언어를 배우는 방식과의 차이를 생각해보며 떠올려봅시다
	- 참고: [XLNet: Generalized Auto-regressive Pre-training for Language Understanding](https://arxiv.org/abs/1906.08237)