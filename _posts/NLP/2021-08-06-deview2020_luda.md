---
layout: post
title: TIL / DEVIEW2020 챗봇 '루다' 육아일기 리뷰
subtitle: 라고 쓰고 Replika라고 읽는다. 그런데 이제 DialogBERT를 곁들인..
gh-repo: ydy8989/ydy8989.github.io
gh-badge: [follow]
categories: [NLP]
tags: [chatbot, nlp]
comments: true
---

오픈도메인 챗봇인 이루다 발표를 리뷰하려 한다. DEVIEW2019에서 발표하셨던 DialogBERT 이후부터 루다를 개발하기 전까지의 과정을 담은 발표이다. 



# Intro

## 두 가지 챗봇 영역

1. 목적 지향형 챗봇
	1. 특정 주제, 편의성, 비서 역할
		시리나 클로바처럼 내가 알고 싶은 질문이나 명령을 수행하기 위한 챗봇
2. 오픈 도메인 챗봇
	1. 자유 주제, 소셜 니즈, 친구 역할
		진짜 대화를 하는 거 자체가 소셜니즈를 채워주는 친구같은 역할을 함

<br/>

## 오픈 도메인 대화가 어려운 이유

- one-to-many 문제
	- 하나의 컨텍스트에 서로 다르지만 좋은 답변이 다수 존재함
	- 마찬가지로, 다양한 유형의 오답도 존재함.
		- 문맥을 이해 못하거나, 말도안되는 말들
	- 훈련이 어렵다
- 무한한 컨텍스트
	- 대화 주제(음식, 영화, 직장 등..)도 무한하고, 문장 유형도(질문, 반응, 제안, 의견 등..) 무한하다.
- 부족한 대화 데이터
	- 대부분의 대량의 언어 데이터셋은 문어체
	- 화자, 턴 등의 대화만의 특수성을 반영한 데이터 셋을 대량으로 구매하기 어렵다. 



## deview 2019 그 이후..

- deview 2019 : dialog-bert
	- 턴 구분 토큰 추가
	- 턴 임베딩 추가
	- 일상대화 데이터로 학습
- 이거도 살펴보기



# Dialogbert 부터 오픈도메인 챗봇 '루다'까지..

## 루다 alpha : XiaoIce 기반 프레임워크

- 루다의 시작과 샤오아이스
- 목표 :
	- 엄청 많은 사람들과 (100만명 이상)
	- 엄청 많은 대화(하루 20턴 이상)
	- 엄청 오랜 기간 동안(3년 이상)



### 루다의 시작과 XiaoIce

영감을 준 논문 : 

- '[The Design and Implementation of XiaoIce, an Empathetic Social Chatbot](https://arxiv.org/abs/1812.08989)'



### 마이크로소프트의 XiaoIce

- 루다 알파버전의 기반이 됨



## 루다 알파 프레임워크

![image](https://user-images.githubusercontent.com/38639633/128527723-b68e0b34-a44e-4235-a1b2-241bedf8512d.png){.width="80%"}{.center}

- 좌측 : NLU
- 가운데 : Retrieval
- 우측 : Ranker 



### NLU 파트(Natural Language Understanding)

- 진행중인 대화를 이해하는 부분
- DialogBERT, Emotin, Dialogue Act, Engage Mode, Topic
- ![image](https://user-images.githubusercontent.com/38639633/128528156-8755b7ef-d501-4dc4-98db-936360f63154.png){.width="80%"}{.center}



### Retrieval : Session DB

- 1차 응답 후보를 가져오는 부분

- 세션DB : 

	- 세션 + 답변 형태로 구성된 DB

	- 현재 대화와 유사한 세션의 응답을 후보로 선정

	- ![image](https://user-images.githubusercontent.com/38639633/128528512-4e2427b6-85a9-46c3-9169-0eb012a75351.png){.width="80%"}{.center}

		

### Retrieval : Content DB

- 1차 응답 후보를 가져오는 부분

- Content DB : 

	- 주제어를 포함한 문장 형태로 구성된 DB
	- 현재 대화의 주제어를 포함한 응답을 후보로 선정
	- ![image](https://user-images.githubusercontent.com/38639633/128528860-5de1dc5e-e4fb-4e12-83fa-1e1a0c6a91dc.png){.width="80%"}{.center}

	



### Ranker

- 응답 후보 중 가장 적합한 말을 선정하는 부분
- Response selection(poly-encoder), Discourse Matching, 기타 feature로 구성
- ![image](https://user-images.githubusercontent.com/38639633/128598741-9b426e6a-10f6-46e6-9acd-8abe99c67efd.png){.width="80%"}{.center}
- 랭커의 후보를 추리는 과정에서 3번째 답이 알맞음을 판단함



### 루다 알파 성능 평가(SSA-Sensibleness & Specificity Average)

- 구글에서 오픈 도메인 챗봇의 성능측정을 위해 제시한 지표
- Sensibleness : 응답이 말이 되는지를 판단
- Specificity : 응답이 구체적인지를 판단
- 위 두 가지 기준을 binary로 판단하게 된다. 
- ![image](https://user-images.githubusercontent.com/38639633/128598819-62811943-9538-4940-af49-f22cda83dde6.png){.width="80%"}{.center}
- 루다의 경우 69%의 sensibleness, 49%의 specificity, 총 59%의 SSA를 달성하였다.



## 루다 베타 : Retrieve & Rank

### 업데이트 1. 프레임워크의 복잡도를 줄이자.

- NLU 세부 모듈 및 Ranker의 여러 피쳐
- 성능 :arrow_double_up:< 복잡도:arrow_double_up:+computing power :arrow_double_up:
- 약간의 성능 손실을 감수하더라도 복잡도를 크게 줄이는 것이 좋은 트레이드 오프라고 판단하였음
- ![image](https://user-images.githubusercontent.com/38639633/128598980-be5ccd20-65ed-47cc-83bd-724230fc08f6.png){.width="80%"}{.center}
	- NLU 파트에서 DialogBERT만 남기고 다 삭제함
	- Ranker파트에서는 response selection만 남겼음

### 업데이트 2. 더 좋은 DialogBERT를 학습시키자

- 첫 DialogBERT의 구조적인 한계 개선
- BERT 기반 Pretraining model 최신 연구 반영
- ![image](https://user-images.githubusercontent.com/38639633/128599029-0ee41b81-e354-4108-b5d0-720c6827cfaa.png){.width="80%"}{.center}
	- Next sentence prediction : 2문장을 주고, 첫번째 문장 바로 다음에 두번째 문장이 올지를 예측하는 방식
	- sentence order prediction : 두 문장 a, b가 있을 때, a\|b가 맞느지, b\a가 맞는지를 예측하는 방식



### 업데이트 3. 응답 DB 퀄리티를 높이자

- Session DB, Content DB 통합 >> 단일 응답 DB
- DB 퀄리티 개선 : 중복문장, 퀄리티 낮은 문장, 페르소나에 맞지 않는 응답 제거
- ![image](https://user-images.githubusercontent.com/38639633/128602148-75ffbd0e-9d69-4535-9450-bfeb1fbf1e40.png){.width="80%"}{.center}
	- 20대 여자 대학생이라는 페르소나에 안맞는 말 제거



### 업데이트 4. 응답 후보를 의미 기반으로 뽑자

- 기존 : TF-IDF류의 lexical 요소를 기반으로 응답 후보를 선정
	- 업데이트 : 대화 임베딩과 응답 후보 임베딩의 cosine similarity로 응답 후보 선정
- 응답 후보에 대해서는 미리 임베딩 계산
	- retrieval 방식의 db이기 때문에, 미리 가지고 있는 데이터에 대해서 임베딩을 다 구해놓고, 실제 서빙시에는 대화 컨텍스트만 임베딩해서 빠르게 서빙할 수 있게하였음
- 차원 축소를 통해 1억개 이상의 응답 후보에서도 서빙 가능하도록 구현



### 업데이트 5. Fine tuning으로 Response selection 성능 개선

- poly-encoder를 개량함 : 직전 턴, 같은 화자의 턴 별도 학습
- Fine-tuning :  24,000세션에 대해 응답 후보에 대한 SSA 레이블링 데이터로 학습

![image](https://user-images.githubusercontent.com/38639633/128602377-dfd9e0f6-b4a1-4b7e-a629-917e16f3522d.png){.width="80%"}{.center}



### 최종 성능

![image](https://user-images.githubusercontent.com/38639633/128602389-aba0777a-ceb6-433a-94e7-cb64ed1b5d02.png){.width="80%"}{.center}

